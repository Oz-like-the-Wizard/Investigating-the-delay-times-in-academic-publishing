{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json\n",
    "from statistics import multimode\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer.high_level import extract_text\n",
    "#import PyPDF2 as pdf2\n",
    "\n",
    "import pycountry\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import snowballstemmer\n",
    "from keybert import KeyBERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r date_df_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIABLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aruba',\n",
       " 'Afghanistan',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Åland Islands',\n",
       " 'Albania',\n",
       " 'Andorra',\n",
       " 'United Arab Emirates',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'American Samoa',\n",
       " 'Antarctica',\n",
       " 'French Southern Territories',\n",
       " 'Antigua and Barbuda',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Burundi',\n",
       " 'Belgium',\n",
       " 'Benin',\n",
       " 'Bonaire, Sint Eustatius and Saba',\n",
       " 'Burkina Faso',\n",
       " 'Bangladesh',\n",
       " 'Bulgaria',\n",
       " 'Bahrain',\n",
       " 'Bahamas',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Saint Barthélemy',\n",
       " 'Belarus',\n",
       " 'Belize',\n",
       " 'Bermuda',\n",
       " 'Bolivia, Plurinational State of',\n",
       " 'Brazil',\n",
       " 'Barbados',\n",
       " 'Brunei Darussalam',\n",
       " 'Bhutan',\n",
       " 'Bouvet Island',\n",
       " 'Botswana',\n",
       " 'Central African Republic',\n",
       " 'Canada',\n",
       " 'Cocos (Keeling) Islands',\n",
       " 'Switzerland',\n",
       " 'Chile',\n",
       " 'China',\n",
       " \"Côte d'Ivoire\",\n",
       " 'Cameroon',\n",
       " 'Congo, The Democratic Republic of the',\n",
       " 'Congo',\n",
       " 'Cook Islands',\n",
       " 'Colombia',\n",
       " 'Comoros',\n",
       " 'Cabo Verde',\n",
       " 'Costa Rica',\n",
       " 'Cuba',\n",
       " 'Curaçao',\n",
       " 'Christmas Island',\n",
       " 'Cayman Islands',\n",
       " 'Cyprus',\n",
       " 'Czechia',\n",
       " 'Germany',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Denmark',\n",
       " 'Dominican Republic',\n",
       " 'Algeria',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'Eritrea',\n",
       " 'Western Sahara',\n",
       " 'Spain',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Finland',\n",
       " 'Fiji',\n",
       " 'Falkland Islands (Malvinas)',\n",
       " 'France',\n",
       " 'Faroe Islands',\n",
       " 'Micronesia, Federated States of',\n",
       " 'Gabon',\n",
       " 'United Kingdom',\n",
       " 'Georgia',\n",
       " 'Guernsey',\n",
       " 'Ghana',\n",
       " 'Gibraltar',\n",
       " 'Guinea',\n",
       " 'Guadeloupe',\n",
       " 'Gambia',\n",
       " 'Guinea-Bissau',\n",
       " 'Equatorial Guinea',\n",
       " 'Greece',\n",
       " 'Grenada',\n",
       " 'Greenland',\n",
       " 'Guatemala',\n",
       " 'French Guiana',\n",
       " 'Guam',\n",
       " 'Guyana',\n",
       " 'Hong Kong',\n",
       " 'Heard Island and McDonald Islands',\n",
       " 'Honduras',\n",
       " 'Croatia',\n",
       " 'Haiti',\n",
       " 'Hungary',\n",
       " 'Indonesia',\n",
       " 'Isle of Man',\n",
       " 'India',\n",
       " 'British Indian Ocean Territory',\n",
       " 'Ireland',\n",
       " 'Iran, Islamic Republic of',\n",
       " 'Iraq',\n",
       " 'Iceland',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Jersey',\n",
       " 'Jordan',\n",
       " 'Japan',\n",
       " 'Kazakhstan',\n",
       " 'Kenya',\n",
       " 'Kyrgyzstan',\n",
       " 'Cambodia',\n",
       " 'Kiribati',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Korea, Republic of',\n",
       " 'Kuwait',\n",
       " \"Lao People's Democratic Republic\",\n",
       " 'Lebanon',\n",
       " 'Liberia',\n",
       " 'Libya',\n",
       " 'Saint Lucia',\n",
       " 'Liechtenstein',\n",
       " 'Sri Lanka',\n",
       " 'Lesotho',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Latvia',\n",
       " 'Macao',\n",
       " 'Saint Martin (French part)',\n",
       " 'Morocco',\n",
       " 'Monaco',\n",
       " 'Moldova, Republic of',\n",
       " 'Madagascar',\n",
       " 'Maldives',\n",
       " 'Mexico',\n",
       " 'Marshall Islands',\n",
       " 'North Macedonia',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Myanmar',\n",
       " 'Montenegro',\n",
       " 'Mongolia',\n",
       " 'Northern Mariana Islands',\n",
       " 'Mozambique',\n",
       " 'Mauritania',\n",
       " 'Montserrat',\n",
       " 'Martinique',\n",
       " 'Mauritius',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Mayotte',\n",
       " 'Namibia',\n",
       " 'New Caledonia',\n",
       " 'Niger',\n",
       " 'Norfolk Island',\n",
       " 'Nigeria',\n",
       " 'Nicaragua',\n",
       " 'Niue',\n",
       " 'Netherlands',\n",
       " 'Norway',\n",
       " 'Nepal',\n",
       " 'Nauru',\n",
       " 'New Zealand',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Panama',\n",
       " 'Pitcairn',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Palau',\n",
       " 'Papua New Guinea',\n",
       " 'Poland',\n",
       " 'Puerto Rico',\n",
       " \"Korea, Democratic People's Republic of\",\n",
       " 'Portugal',\n",
       " 'Paraguay',\n",
       " 'Palestine, State of',\n",
       " 'French Polynesia',\n",
       " 'Qatar',\n",
       " 'Réunion',\n",
       " 'Romania',\n",
       " 'Russian Federation',\n",
       " 'Rwanda',\n",
       " 'Saudi Arabia',\n",
       " 'Sudan',\n",
       " 'Senegal',\n",
       " 'Singapore',\n",
       " 'South Georgia and the South Sandwich Islands',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha',\n",
       " 'Svalbard and Jan Mayen',\n",
       " 'Solomon Islands',\n",
       " 'Sierra Leone',\n",
       " 'El Salvador',\n",
       " 'San Marino',\n",
       " 'Somalia',\n",
       " 'Saint Pierre and Miquelon',\n",
       " 'Serbia',\n",
       " 'South Sudan',\n",
       " 'Sao Tome and Principe',\n",
       " 'Suriname',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Sweden',\n",
       " 'Eswatini',\n",
       " 'Sint Maarten (Dutch part)',\n",
       " 'Seychelles',\n",
       " 'Syrian Arab Republic',\n",
       " 'Turks and Caicos Islands',\n",
       " 'Chad',\n",
       " 'Togo',\n",
       " 'Thailand',\n",
       " 'Tajikistan',\n",
       " 'Tokelau',\n",
       " 'Turkmenistan',\n",
       " 'Timor-Leste',\n",
       " 'Tonga',\n",
       " 'Trinidad and Tobago',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Tuvalu',\n",
       " 'Taiwan, Province of China',\n",
       " 'Tanzania, United Republic of',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United States Minor Outlying Islands',\n",
       " 'Uruguay',\n",
       " 'United States',\n",
       " 'Uzbekistan',\n",
       " 'Holy See (Vatican City State)',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Venezuela, Bolivarian Republic of',\n",
       " 'Virgin Islands, British',\n",
       " 'Virgin Islands, U.S.',\n",
       " 'Viet Nam',\n",
       " 'Vanuatu',\n",
       " 'Wallis and Futuna',\n",
       " 'Samoa',\n",
       " 'Yemen',\n",
       " 'South Africa',\n",
       " 'Zambia',\n",
       " 'Zimbabwe',\n",
       " 'USA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cntry.name for cntry in list(pycountry.countries)] + [\"USA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Univ. JSON Related Variables:\n",
    "\n",
    "# Uni Email Codes-Country dict:\n",
    "# Uni Names-Country dict:\n",
    "\n",
    "def uni_dict_creator():\n",
    "    with open('world_universities_and_domains.json', encoding=\"utf8\") as fp:\n",
    "        uni_dict = json.load(fp)\n",
    "\n",
    "    uni_df = pd.DataFrame(uni_dict)\n",
    "\n",
    "    uni_code_country_dict = {}\n",
    "    uni_name_country_dict = {}\n",
    "\n",
    "    for index, row in uni_df.iterrows():\n",
    "        uni_code_country_dict.update({dom : row.country for dom in row.domains})\n",
    "        uni_name_country_dict.update({row[\"name\"] : row.country})\n",
    "\n",
    "    del uni_name_country_dict[\"University of Technology\"]\n",
    "\n",
    "    return uni_code_country_dict, uni_name_country_dict\n",
    "\n",
    "#for test reasons:\n",
    "uni_code_country_dict, uni_name_country_dict = uni_dict_creator()\n",
    "\n",
    "# pycountry Related Variables:\n",
    "# Country Name list:\n",
    "cntry_name_list = [cntry.name for cntry in list(pycountry.countries)] + [\"USA\"]\n",
    "# Country Email Code (alpha 2)-Country Dict:\n",
    "cntry_code_dict = {cntry.alpha_2.lower() : cntry.name for cntry in list(pycountry.countries)}\n",
    "\n",
    "\n",
    "# REGEX PATTERNS:\n",
    "# Email Regex Pattern:\n",
    "email_reg = re.compile(\"[a-zA-Z0-9._-]+@([a-zA-Z0-9_-]+\\.((?:[a-zA-Z0-9_-]+\\.)*([a-zA-Z0-9_-]+)))\")\n",
    "# Uni Names Regex Pattern:\n",
    "uni_name_regex = re.compile(\"(?=(\"+'|'.join(list(uni_name_country_dict.keys()))+r\"))\")\n",
    "# Country Name Regex Pattern:\n",
    "country_name_regex = re.compile(\"(?=(\"+'|'.join(cntry_name_list)+r\"))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- E-Mail(Uni Domain) - DONE! :\n",
    "\n",
    "def email_converter(pdf_read):\n",
    "    \n",
    "    email_caught = email_reg.search(pdf_read)\n",
    "\n",
    "    try:\n",
    "        return uni_code_country_dict[email_caught.group(1).lower()]\n",
    "    except AttributeError:\n",
    "        return np.nan\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return uni_code_country_dict[email_caught.group(2).lower()]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return cntry_code_dict[email_caught.group(3)]\n",
    "            except KeyError:\n",
    "                cntry_pttrn =  email_caught.group(3)[:3]\n",
    "\n",
    "                if cntry_pttrn in [\"com\",\"net\"]:\n",
    "                    return np.nan\n",
    "\n",
    "                elif cntry_pttrn in [\"edu\",\"gov\"]:\n",
    "                    return \"United States\"\n",
    "\n",
    "                if len(cntry_pttrn)==3:\n",
    "                    cntry_pttrn = cntry_pttrn[:-1]\n",
    "\n",
    "                try:\n",
    "                    return cntry_code_dict[cntry_pttrn]\n",
    "                except:\n",
    "                    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_converter(email_testo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Country Match - DONE:\n",
    "def country_converter(pdf_read):\n",
    "    try:\n",
    "        cntry_match = country_name_regex.search(pdf_read)[1]\n",
    "        return  cntry_match if cntry_match != \"USA\" else \"United States\"\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return country_name_regex.search(pdf_read.title())[1]\n",
    "        except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- University Match - DONE! :\n",
    "def univ_converter(pdf_read):   \n",
    "    try:\n",
    "        uni_found = uni_name_regex.search(pdf_read)[1]\n",
    "        return uni_name_country_dict[uni_found]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affiliation(pdf_read):\n",
    "    email_affl = email_converter(pdf_read)\n",
    "    uni_affl = univ_converter(pdf_read)\n",
    "    cntry_affl = country_converter(pdf_read)\n",
    "\n",
    "    affl_vote = multimode([email_affl,uni_affl,cntry_affl])\n",
    "\n",
    "    if len(affl_vote) == 1:\n",
    "        return affl_vote[0]\n",
    "    elif not np.nan(email_affl):\n",
    "        return email_affl\n",
    "    elif not np.nan(uni_affl):\n",
    "        return uni_affl\n",
    "    else:\n",
    "        return cntry_affl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three Affiliation capture funcitons are ready! Required variables are:\n",
    "\n",
    "* \"World Unis\" json & pycountry\n",
    "* E-mail fn:\n",
    "    - email_reg_modified\n",
    "    - uni_code_country_dict\n",
    "    - country_code_dict\n",
    "* Country fn:\n",
    "    - cntry_name_list\n",
    "* Uni fn:\n",
    "    - uni_name_list\n",
    "    - uni_name_country_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATE REGEXES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX PATTERNS - RECEIVED & SUBMITTED\n",
    "\n",
    "# 1 - 10 MAY 1960:\n",
    "rec_1 = re.compile(\"(?:(?:received|submitted)\\s?(?:on|date)?\\s?\\:?\\s?)(\\d{1,2}\\s*(Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober)?|Nov(ember|.)?|Dec(ember|.)?)\\s*(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 2 - MAY 10, 1960\n",
    "rec_2 = re.compile(\"(?:(?:received|submitted)\\s?(?:on|date)?\\s?\\:?\\s?)((Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober|.)?|Nov(ember|.)?|Dec(ember|.)?)\\s?\\d{1,2}\\,?\\s?(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 3 - MAY 1960\n",
    "rec_3 = re.compile(\"(?:(?:received|submitted)\\s?(?:on|date)?\\s?\\:?\\s?)((Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober|.)?|Nov(ember|.)?|Dec(ember|.)?)\\s*(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 4 - 10/05/1960 OR 10.05.1960 OR 10/05/1960\n",
    "rec_4 = re.compile(\"(?:(?:received|submitted)\\s?(?:on|date)?\\s?\\:?\\s?)([1-3][0-9](\\.|\\-|\\/|\\:)?[0-1][0-9](\\.|\\-|\\/\\:)?(19|20)\\d{2})\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_date_capture(pdf_read):\n",
    "\n",
    "    rec_patterns = [rec_1, rec_2, rec_3, rec_4]\n",
    "\n",
    "    for pattern in rec_patterns:\n",
    "        try:\n",
    "            date = pattern.search(pdf_read).group(1)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            return date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX PATTERNS - ACCEPTED\n",
    "\n",
    "# 1 - 10 MAY 1960:\n",
    "acc_1 = re.compile(\"(?:(?:accepted)\\s?(?:on|date|for publication on)?\\s?\\:?\\s?)(\\d{1,2}\\s*(Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober|.)?|Nov(ember|.)?|Dec(ember|.)?)\\s*(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 2 - MAY 10, 1960\n",
    "acc_2 = re.compile(\"(?:(?:accepted)\\s?(?:on|date|for publication on)?\\s?\\:?\\s?)((Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober|.)?|Nov(ember|.)?|Dec(ember|.)?)\\s?\\d{1,2}\\,?\\s?(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 3 - MAY 1960\n",
    "acc_3 = re.compile(\"(?:(?:accepted)\\s?(?:on|date|for publication on)?\\s?\\:?\\s?)((Jan(uary|.)?|Feb(ruary|.)?|Mar(ch|.)?|Apr(il|.)?|May|Jun(e|.)?|Jul(y|.)?|Aug(ust|.)?|Sep(tember|.)?|Oct(ober|.)?|Nov(ember|.)?|Dec(ember|.)?)\\s*(19|20)\\d{2})\", re.IGNORECASE)\n",
    "\n",
    "# 4 - 10/05/1960 OR 10.05.1960 OR 10/05/1960\n",
    "acc_4 = re.compile(\"(?:(?:accepted)\\s?(?:on|date|for publication on)?\\s?\\:?\\s?)([1-3][0-9](\\.|\\-|\\/|\\:)?[0-1][0-9](\\.|\\-|\\/\\:)?(19|20)\\d{2})\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_date_capture(pdf_text):\n",
    "    \n",
    "    acc_patterns = [acc_1, acc_2, acc_3, acc_4]\n",
    "    \n",
    "    for pattern in acc_patterns:\n",
    "        try:\n",
    "            date = pattern.search(pdf_text).group(1)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF WORKS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Preprocessing:\n",
    "def prep_simple(pdf_read):\n",
    "    pdf_read = pdf_read.replace(\"\\n\",\" \")\n",
    "    pdf_read = re.sub(' +', ' ', pdf_read)\n",
    "    return pdf_read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(pdf_read):\n",
    "    # Removing \"References\":\n",
    "    pdf_modified = \"\".join(pdf_read.split(\"references\")[:-1])\n",
    "    \n",
    "    if len(pdf_modified) > 0:\n",
    "        return pdf_modified \n",
    "    else:\n",
    "        pdf_modified = \"\".join(pdf_read.split(\"acknowledgements\")[:-1])\n",
    "        if len(pdf_modified) > 0:\n",
    "            return pdf_modified \n",
    "        else:\n",
    "            return pdf_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Preprocessing:\n",
    "\n",
    "# pdf_read -> pdf_modified\n",
    "\n",
    "def prep_main(pdf_modified):\n",
    "    # Lowercase all:\n",
    "    pdf_modified = pdf_modified.lower()\n",
    "    # Removing \"References\":\n",
    "    pdf_modified = remove_references(pdf_modified)\n",
    "    # Remove URL:\n",
    "    pdf_modified = re.sub(r'http\\S+', '', pdf_modified)\n",
    "    # Remove emails:\n",
    "    pdf_modified = re.sub(r\"\\S*@\\S*\\s?\", \"\", pdf_modified)\n",
    "    # Remove everything in brackets & paranthesis:\n",
    "    pdf_modified = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", pdf_modified)\n",
    "    # Remove punctuation:\n",
    "    pdf_modified = re.sub(r'[^\\w\\s]', '', pdf_modified)\n",
    "    # Remove numbers\n",
    "    pdf_modified = re.sub(r'[0-9]', '', pdf_modified)\n",
    "    # Remove HTML tags:\n",
    "    pdf_modified = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', pdf_modified)\n",
    "    # Remove single letters:\n",
    "    pdf_modified = re.sub(r\"(?<=(^|\\s))\\D(\\s|$)\",\"\", pdf_modified)\n",
    "    # Remove multiple spaces:\n",
    "    pdf_modified = re.sub(' +', ' ', pdf_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join(\"hasjkdhassasdgas\".split(\"references\")[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING AREA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random pdf_read:\n",
    "\n",
    "random_dir_url = date_df_ok[date_df_ok.Rec_date.notna()].direct_url.sample(1).values[0]\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36'}\n",
    "\n",
    "pdf_req = requests.get(random_dir_url,headers=HEADERS)\n",
    "\n",
    "print(pdf_req)\n",
    "pdf_io = io.BytesIO(pdf_req.content)\n",
    "pdf_read = extract_text(pdf_io,page_numbers=[])\n",
    "\n",
    "print(random_dir_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(pdf_read):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(pdf_read, keyphrase_ngram_range=(1, 1), stop_words=\"english\", top_n=10)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ogi():\n",
    "    var1 = \"vok\" \n",
    "    \n",
    "    def og_printer(self):\n",
    "        print(ogi.var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "og = ogi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vok\n"
     ]
    }
   ],
   "source": [
    "og.og_printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @retry((OgiError),tries=5, delay=75)\n",
    "    @retry((FourZeroFourError),tries=5, delay=15)\n",
    "    def get_pdf_bytes(self, pdf_url):\n",
    "\n",
    "        try:\n",
    "            pdf_cont = self.sess.get(pdf_url, verify=False, timeout=15)\n",
    "\n",
    "            if pdf_cont.status_code == 404:\n",
    "                self._change_base_url()\n",
    "                raise FourZeroFourError(\"Threat Level: 404\")\n",
    "\n",
    "        except:\n",
    "            self._change_base_url()\n",
    "            raise FourZeroFourError(f\"Can't connect to direct URL: {pdf_url}\")\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                pdf_bytes = io.BytesIO(pdf_cont.content)\n",
    "                return pdf_bytes\n",
    "            except:\n",
    "                self._change_base_url()\n",
    "                raise FourZeroFourError(\"can't get bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @retry((OgiError), tries=3, delay=5)\n",
    "    def pdf_reader(self,pdf_bytes):\n",
    "        try:\n",
    "            pdf_read = extract_text(pdf_bytes)\n",
    "            return pdf_read\n",
    "        except:\n",
    "            self._change_base_url()\n",
    "            raise OgiError(\"can't read pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_all(self, identifier):\n",
    "\n",
    "        try:\n",
    "            pdf_bytes = self.get_pdf_bytes(identifier)       \n",
    "        except FourZeroFourError:\n",
    "            return \"direct_url_error\"\n",
    "\n",
    "        try:\n",
    "            pdf_read = self.pdf_reader(pdf_bytes)\n",
    "            \n",
    "        except OgiError:\n",
    "            return \"cant_read_pdf\"\n",
    "\n",
    "        else:\n",
    "            pdf_read = prep_simple(pdf_read)\n",
    "\n",
    "            dates = self.get_dates(pdf_read)\n",
    "            affiliations = self.get_affiliations(pdf_read)\n",
    "\n",
    "            pdf_read = self.prep_main(pdf_read)\n",
    "            keywords = self.get_keywords(pdf_read)\n",
    "\n",
    "            return dates, affiliations, keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def rec_date_capture(self,pdf_read):\n",
    "        rec_patterns = [self.rec_1, self.rec_2, self.rec_3, self.rec_4]\n",
    "        for pattern in rec_patterns:\n",
    "            try:\n",
    "                date = pattern.search(pdf_read).group(1)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            else:\n",
    "                return date\n",
    "\n",
    "    def acc_date_capture(self,pdf_text):\n",
    "        acc_patterns = [self.acc_1, self.acc_2, self.acc_3, self.acc_4]\n",
    "        for pattern in acc_patterns:\n",
    "            try:\n",
    "                date = pattern.search(pdf_text).group(1)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            else:\n",
    "                return date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(self,pdf_read):\n",
    "    rec_date = self.rec_date_capture(pdf_read)\n",
    "    acc_date = self.acc_date_capture(pdf_read)\n",
    "    return [rec_date, acc_date] if (rec_date != None) | (acc_date != None) else [\"no_date_found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scihub_wartortle import SciHub_watergun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_w = SciHub_watergun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nature.com/articles/eye201520.pdf\n"
     ]
    }
   ],
   "source": [
    "random_dir_url = date_df_ok[date_df_ok.Rec_date.notna()].direct_url.sample(1).values[0]\n",
    "print(random_dir_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['19 July 2014', '19 January 2015'],\n",
       " 'United States',\n",
       " [('vitrectomy', 0.4571),\n",
       "  ('vitreoretinopathy', 0.4481),\n",
       "  ('ophthalmology', 0.4463),\n",
       "  ('retinopathy', 0.4418),\n",
       "  ('melanomas', 0.4251),\n",
       "  ('melanoma', 0.3913),\n",
       "  ('ocular', 0.3882),\n",
       "  ('uveal', 0.3726),\n",
       "  ('cataract', 0.3696),\n",
       "  ('intraocular', 0.3585)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_w.get_all(random_dir_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r date_df_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_dict = [{\"harf\": \"a\", \"rank\": 1},{\"harf\": \"b\", \"rank\": 2},{\"harf\": \"c\", \"rank\": 3},{\"harf\": \"d\", \"rank\": 4}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testofn(rank):\n",
    "    iki = rank % 2\n",
    "    uc = rank % 3\n",
    "    return iki, uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for harf in testo_dict:\n",
    "    ikires, ucres = testofn(harf[\"rank\"])\n",
    "    harf[\"ikires\"] = ikires\n",
    "    harf[\"uc\"] = ucres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'harf': 'a', 'rank': 1, 'ikires': 1, 'uc': 1},\n",
       " {'harf': 'b', 'rank': 2, 'ikires': 0, 'uc': 2},\n",
       " {'harf': 'c', 'rank': 3, 'ikires': 1, 'uc': 0},\n",
       " {'harf': 'd', 'rank': 4, 'ikires': 0, 'uc': 1}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testo_dict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92ba50c92c9dc11b366869717e90d544d23b7140e20708921d0ff91f276d2e3f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
