{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import regex as re\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "# from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "\n",
    "# from pybliometrics.scopus import AuthorSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restarted the PC, re-loading variables:\n",
    "\n",
    "with open(\"new_author_out_list\", \"rb\") as fp:\n",
    "    new_author_out_list = pickle.load(fp)\n",
    "\n",
    "with open(\"new_first_author_df\", \"rb\") as fp:\n",
    "    new_first_author_df = pickle.load(fp)\n",
    "\n",
    "# with open(\"new_author_out_list_loop_save\", \"rb\") as fp:\n",
    "#     new_author_out_list_loop_save = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restarted the PC, re-loading variables:\n",
    "\n",
    "# with open(\"k5k_author_out_list\", \"rb\") as fp:\n",
    "#     k5k_author_out_list = pickle.load(fp)\n",
    "\n",
    "with open(\"k5k_first_author_df\", \"rb\") as fp:\n",
    "    k5k_first_author_df = pickle.load(fp)\n",
    "\n",
    "# with open(\"new_author_out_list_loop_save\", \"rb\") as fp:\n",
    "#     new_author_out_list_loop_save = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_codes_dict = {'Agricultural and Biological Sciences': 'AGRI',\n",
    " 'Arts and Humanities': 'ARTS',\n",
    " 'Biochemistry, Genetics and Molecular Biology': 'BIOC',\n",
    " 'Business, Management and Accounting': 'BUSI',\n",
    " 'Chemical Engineering': 'CENG',\n",
    " 'Chemistry': 'CHEM',\n",
    " 'Computer Science': 'COMP',\n",
    " 'Decision Sciences': 'DECI',\n",
    " 'Dentistry': 'DENT',\n",
    " 'Earth and Planetary Sciences': 'EART',\n",
    " 'Economics, Econometrics and Finance': 'ECON',\n",
    " 'Energy': 'ENER',\n",
    " 'Engineering': 'ENGI',\n",
    " 'Environmental Science': 'ENVI',\n",
    " 'Health Professions': 'HEAL',\n",
    " 'Immunology and Microbiology': 'IMMU',\n",
    " 'Materials Science': 'MATE',\n",
    " 'Mathematics': 'MATH',\n",
    " 'Medicine': 'MEDI',\n",
    " 'Neuroscience': 'NEUR',\n",
    " 'Nursing': 'NURS',\n",
    " 'Pharmacology, Toxicology and Pharmaceutics': 'PHAR',\n",
    " 'Physics and Astronomy': 'PHYS',\n",
    " 'Psychology': 'PSYC',\n",
    " 'Social Sciences': 'SOCI',\n",
    " 'Veterinary': 'VETE',\n",
    " 'Multidisciplinary': 'MULT'}\n",
    "\n",
    "def sbj_area_query_creator(sbj_list):\n",
    "    query = \" AND \".join([f\"SUBJAREA({scopus_codes_dict[sbj]})\" for sbj in sbj_list])\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the Scopus API step, will try to start and create a second loop in this nb.\n",
    "\n",
    "Will use new_additions_df as the source article dataset to avoid duplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: author_df & first_author_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7e_variables/new_additions_df\", \"rb\") as fp:\n",
    "    new_additions_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_df = new_additions_df.dropna(subset= \"author\").reset_index().loc[:,[\"DOI\", \"JRNL_ID\", \"author\"]].explode(\"author\").reset_index(drop=True)\n",
    "\n",
    "new_aut_df = pd.DataFrame(list(new_author_df['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the two datasets:\n",
    "new_author_df = pd.concat([new_author_df, new_aut_df], axis=1)\n",
    "\n",
    "new_author_df.drop([\"suffix\", \"name\", \"author\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty given & family:\n",
    "new_author_df.dropna(subset=[\"family\", \"given\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to df[0] = df[0].str.strip() first:\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "# Remove multiple spaces:\n",
    "new_author_df.given = new_author_df.given.map(lambda x: re.sub(' +', ' ', x))\n",
    "new_author_df.family = new_author_df.family.map(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "\n",
    "# Case 1: A. C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x)))].given.map(lambda x: x +\".\")\n",
    "# Case 2: A.C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x)))].given.map(lambda x: x.replace(\".\",\". \") +\".\")\n",
    "# Case 3: AC\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x)))].given.map(lambda x: x[0] + \". \" + x[1] + \".\")\n",
    "# Case 4: A C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x)))].given.map(lambda x: x.replace(\" \", \". \") + \".\")\n",
    "# Case 5: A.C.\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x))), \"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x)))].given.map(lambda x: x.replace(\".\", \". \").strip())\n",
    "\n",
    "# Single letter names:\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x)))].given.map(lambda x: x + \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing [Name], (Name),\n",
    "# remove \"Name\" , “Name”,\n",
    "# remove & % , - ' .\n",
    "# simplf. ö & ü & � & i\n",
    "\n",
    "# Main Preprocessing:\n",
    "def name_simplifier(pdf_read):\n",
    "    # Lowercase all:\n",
    "    pdf_modified = pdf_read.lower()\n",
    "    # Remove everything in brackets & paranthesis:\n",
    "    pdf_modified = re.sub(\"[\\(\\[\\{].*?[\\)\\]\\}]\", \"\", pdf_modified)\n",
    "    # Remove quote names:\n",
    "    pdf_modified = re.sub('[\"“].*?[\"”]', \"\", pdf_modified)\n",
    "    # Remove numbers\n",
    "    pdf_modified = re.sub(r'[0-9]', '', pdf_modified)\n",
    "    # Remove weird punct.\n",
    "    pdf_modified = re.sub(r'[&\\?\\$\\+\\\\\\*\\^\\|]', '', pdf_modified)\n",
    "    # Simplify acct. a:\n",
    "    pdf_modified = re.sub(r'[áạàảãăặằẳẵâấậầẩẫā]', 'a', pdf_modified)\n",
    "    # Simplify acct. i:\n",
    "    pdf_modified = re.sub(r'[íịìỉĩïǐĭīĩįɨıî]', 'i', pdf_modified)\n",
    "    # Simplify acct. i:\n",
    "    pdf_modified = re.sub(r'[éẹèẻẽêếệềểễ]', 'e', pdf_modified)\n",
    "    # Simplify acct. o:\n",
    "    pdf_modified = re.sub(r'[óòȯôöǒŏōõǫőốồøṓṑ]', 'o', pdf_modified)\n",
    "    # Simplify acct. u:\n",
    "    pdf_modified = re.sub(r'[úùûüǔŭūũů]', 'u', pdf_modified)\n",
    "    # Remove multiple spaces:\n",
    "    pdf_modified = re.sub(' +', ' ', pdf_modified)\n",
    "    # Return pdf_modified\n",
    "    return pdf_modified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpli:\n",
    "\n",
    "new_author_df.loc[:,\"new_given\"] = new_author_df.given.map(name_simplifier)\n",
    "new_author_df.loc[:,\"new_family\"] = new_author_df.family.map(name_simplifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After process stripping:\n",
    "\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "new_author_df.new_given = new_author_df.new_given.str.strip()\n",
    "new_author_df.new_family = new_author_df.new_family.str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)\n",
    "\n",
    "new_author_df = pd.merge(left=new_author_df, right=q1_jrnl_df[\"SA_list\"], how=\"left\", left_on=\"JRNL_ID\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a first author df\n",
    "# Using .groupby() to have DOI & SA_list columns \n",
    "\n",
    "\n",
    "new_first_author_df = new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"SA_list\"].apply(list)\n",
    "new_first_author_df = new_first_author_df.to_frame().merge(new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"DOI\"].apply(list), how= \"inner\", right_index=True, left_index=True)\n",
    "new_first_author_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24266</th>\n",
       "      <td>‘fauxparl’</td>\n",
       "      <td></td>\n",
       "      <td>[[Health Professions, Social Sciences]]</td>\n",
       "      <td>[10.1080/09687599.2020.1838262]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        new_given new_family                                  SA_list  \\\n",
       "24266  ‘fauxparl’             [[Health Professions, Social Sciences]]   \n",
       "\n",
       "                                   DOI  \n",
       "24266  [10.1080/09687599.2020.1838262]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_first_author_df[new_first_author_df.new_given.map(lambda x: bool(re.match(\"\\W\",x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [new_given, new_family, SA_list, DOI]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_first_author_df[new_first_author_df.new_family.map(lambda x: bool(re.match(\"\\W\",x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problematic DOI: 10.1080/09687599.2020.1838262\n",
    "\n",
    "new_first_author_df.drop(24266, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Flattening SA_list col:\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "new_first_author_df.loc[:,\"SA_list\"] = new_first_author_df.SA_list.map(flatten)\n",
    "\n",
    "# 2- Creating an Author ID col:\n",
    "new_first_author_df.reset_index(drop=True, inplace=True)\n",
    "new_first_author_df.reset_index(drop=False,inplace=True)\n",
    "new_first_author_df.loc[:,\"index\"] = new_first_author_df.index.map(lambda x: \"aut_\" + str(x))\n",
    "new_first_author_df.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_first_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_first_author_df, p)\n",
    "\n",
    "with open(\"new_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_author_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Scopus API Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_input_dict = new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 12:58:34\n",
      "Loop save complete!\n",
      "Time = 16:26:45\n",
      "Changed API key, now using key: b7e95f5ea731eb5c9e84e7a1d499f50e\n",
      "Loop save complete!\n",
      "Time = 16:41:40\n",
      "Changed API key, now using key: 8707db153e4b9672fa6df25b03a5f747\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP 500 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28los%29+AND+AUTHFIRST%28e.+l%E2%80%8A.%29+AND+SUBJAREA%28NEUR%29+AND+SUBJAREA%28BIOC%29+AND+SUBJAREA%28MEDI%29\nand using headers {'X-ELS-APIKey': '8707db153e4b9672fa6df25b03a5f747', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"GENERAL_SYSTEM_ERROR\",\"statusText\":\"Error transforming XML with XSL\"}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b1e70d0f11e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mauth_srch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElsSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mauth_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elssearch.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, els_client, get_all)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m## TODO: add exception handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mapi_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mels_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tot_num_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'opensearch:totalResults'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elsclient.py\u001b[0m in \u001b[0;36mexec_request\u001b[1;34m(self, URL)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\nand using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP 500 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28los%29+AND+AUTHFIRST%28e.+l%E2%80%8A.%29+AND+SUBJAREA%28NEUR%29+AND+SUBJAREA%28BIOC%29+AND+SUBJAREA%28MEDI%29\nand using headers {'X-ELS-APIKey': 'b7e95f5ea731eb5c9e84e7a1d499f50e', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"GENERAL_SYSTEM_ERROR\",\"statusText\":\"Error transforming XML with XSL\"}}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b1e70d0f11e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mauth_srch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElsSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mauth_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elssearch.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, els_client, get_all)\u001b[0m\n\u001b[0;32m     93\u001b[0m             all results for the search, up to a maximum of 5,000.\"\"\"\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m## TODO: add exception handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mapi_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mels_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tot_num_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'opensearch:totalResults'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entry'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elsclient.py\u001b[0m in \u001b[0;36mexec_request\u001b[1;34m(self, URL)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\nand using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP 500 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28los%29+AND+AUTHFIRST%28e.+l%E2%80%8A.%29+AND+SUBJAREA%28NEUR%29+AND+SUBJAREA%28BIOC%29+AND+SUBJAREA%28MEDI%29\nand using headers {'X-ELS-APIKey': '8707db153e4b9672fa6df25b03a5f747', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"GENERAL_SYSTEM_ERROR\",\"statusText\":\"Error transforming XML with XSL\"}}}"
     ]
    }
   ],
   "source": [
    "# Full Loop - First:\n",
    "# Fixed API keys lists\n",
    "\n",
    "new_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"b7e95f5ea731eb5c9e84e7a1d499f50e\",\"8707db153e4b9672fa6df25b03a5f747\",\n",
    "\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "used_api_keys_list = list()\n",
    "\n",
    "client = ElsClient(\"30195f0b2192052a36bcab9ce3c4064f\")\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in new_author_input_dict:\n",
    "    given_in = new_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = new_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = new_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "    except:\n",
    "        if len(avail_api_keys_list) >0:\n",
    "            with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(new_author_out_list, p)\n",
    "            print(\"Loop save complete!\")\n",
    "            new_key = avail_api_keys_list.pop(0)            \n",
    "            client = ElsClient(new_key)\n",
    "            \n",
    "            print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "            \n",
    "            print(f\"Changed API key, now using key: {new_key}\")\n",
    "            used_api_keys_list.append(new_key)\n",
    "\n",
    "            auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "            auth_srch.execute(client)\n",
    "\n",
    "        else:\n",
    "            print(\"No available API keys to use!\")\n",
    "            break\n",
    "\n",
    "\n",
    "    country_out = list()\n",
    "    given_out = list()\n",
    "    family_out = list()\n",
    "    doc_count_out = list()\n",
    "\n",
    "    for auth in auth_srch.results:\n",
    "        # Country:\n",
    "        try:\n",
    "            country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Given:\n",
    "        try:\n",
    "            given_out.append(auth['preferred-name']['given-name'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Family:\n",
    "        try:\n",
    "            family_out.append(auth['preferred-name']['surname'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Doc Count:\n",
    "        try:\n",
    "            doc_count_out.append(auth['document-count'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "\n",
    "    new_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_author_out_list\",\"wb\") as p:\n",
    "    pickle.dump(new_author_out_list, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>country</th>\n",
       "      <th>given</th>\n",
       "      <th>family</th>\n",
       "      <th>doc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aut_0</td>\n",
       "      <td>[United States, China, India, Saudi Arabia, Om...</td>\n",
       "      <td>[Md Zakirul Alam, Md Mahbub, Moshahid Alam, Pr...</td>\n",
       "      <td>[Bhuiyan, Alam, Rizvi, Alam, Khan, Sarker, Ala...</td>\n",
       "      <td>[230, 209, 159, 113, 111, 105, 82, 82, 74, 68,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aut_1</td>\n",
       "      <td>[Spain, Portugal]</td>\n",
       "      <td>[Antonio, Maria Alcina]</td>\n",
       "      <td>[Alcina, Pereira]</td>\n",
       "      <td>[84, 84]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aut_2</td>\n",
       "      <td>[Jordan]</td>\n",
       "      <td>[Abeer]</td>\n",
       "      <td>[AlHadidi]</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aut_3</td>\n",
       "      <td>[United States]</td>\n",
       "      <td>[Andriy, D. A.]</td>\n",
       "      <td>[Andreyev, Andreyev]</td>\n",
       "      <td>[27, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aut_4</td>\n",
       "      <td>[Saudi Arabia]</td>\n",
       "      <td>[Abdulkadir Sh]</td>\n",
       "      <td>[Aydarous]</td>\n",
       "      <td>[27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17019</th>\n",
       "      <td>aut_17021</td>\n",
       "      <td>[Spain]</td>\n",
       "      <td>[Pedro L.]</td>\n",
       "      <td>[Valenzuela]</td>\n",
       "      <td>[159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>aut_17022</td>\n",
       "      <td>[Portugal, Portugal]</td>\n",
       "      <td>[Pedro M., Pedro M.]</td>\n",
       "      <td>[Castro, Castro]</td>\n",
       "      <td>[141, 22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17021</th>\n",
       "      <td>aut_17023</td>\n",
       "      <td>[Portugal, Brazil, Portugal]</td>\n",
       "      <td>[Pedro Mendes, Pedro Walfir Martins E., Pedro M.]</td>\n",
       "      <td>[Sousa, Souza-Filho, Sousa]</td>\n",
       "      <td>[104, 92, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>aut_17024</td>\n",
       "      <td>[Portugal]</td>\n",
       "      <td>[P. M.P.]</td>\n",
       "      <td>[Salomé]</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17023</th>\n",
       "      <td>aut_17025</td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>[Pegah]</td>\n",
       "      <td>[Khaloo]</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17024 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                            country  \\\n",
       "0          aut_0  [United States, China, India, Saudi Arabia, Om...   \n",
       "1          aut_1                                  [Spain, Portugal]   \n",
       "2          aut_2                                           [Jordan]   \n",
       "3          aut_3                                    [United States]   \n",
       "4          aut_4                                     [Saudi Arabia]   \n",
       "...          ...                                                ...   \n",
       "17019  aut_17021                                            [Spain]   \n",
       "17020  aut_17022                               [Portugal, Portugal]   \n",
       "17021  aut_17023                       [Portugal, Brazil, Portugal]   \n",
       "17022  aut_17024                                         [Portugal]   \n",
       "17023  aut_17025                                             [Iran]   \n",
       "\n",
       "                                                   given  \\\n",
       "0      [Md Zakirul Alam, Md Mahbub, Moshahid Alam, Pr...   \n",
       "1                                [Antonio, Maria Alcina]   \n",
       "2                                                [Abeer]   \n",
       "3                                        [Andriy, D. A.]   \n",
       "4                                        [Abdulkadir Sh]   \n",
       "...                                                  ...   \n",
       "17019                                         [Pedro L.]   \n",
       "17020                               [Pedro M., Pedro M.]   \n",
       "17021  [Pedro Mendes, Pedro Walfir Martins E., Pedro M.]   \n",
       "17022                                          [P. M.P.]   \n",
       "17023                                            [Pegah]   \n",
       "\n",
       "                                                  family  \\\n",
       "0      [Bhuiyan, Alam, Rizvi, Alam, Khan, Sarker, Ala...   \n",
       "1                                      [Alcina, Pereira]   \n",
       "2                                             [AlHadidi]   \n",
       "3                                   [Andreyev, Andreyev]   \n",
       "4                                             [Aydarous]   \n",
       "...                                                  ...   \n",
       "17019                                       [Valenzuela]   \n",
       "17020                                   [Castro, Castro]   \n",
       "17021                        [Sousa, Souza-Filho, Sousa]   \n",
       "17022                                           [Salomé]   \n",
       "17023                                           [Khaloo]   \n",
       "\n",
       "                                               doc_count  \n",
       "0      [230, 209, 159, 113, 111, 105, 82, 82, 74, 68,...  \n",
       "1                                               [84, 84]  \n",
       "2                                                   [24]  \n",
       "3                                                [27, 1]  \n",
       "4                                                   [27]  \n",
       "...                                                  ...  \n",
       "17019                                              [159]  \n",
       "17020                                          [141, 22]  \n",
       "17021                                      [104, 92, 31]  \n",
       "17022                                              [100]  \n",
       "17023                                               [20]  \n",
       "\n",
       "[17024 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_input_dict = new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]][16100:].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 02:56:05\n",
      "Loop save complete!\n",
      "Time = 02:56:06\n",
      "Changed API key, now using key: 2dab4694b4347fa574d159bb97484fc4\n",
      "Loop save complete!\n",
      "Time = 03:35:20\n",
      "Changed API key, now using key: 6af3d2c09eb08ec6e12f0cead9a1f5bb\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP 400 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28or%29+AND+AUTHFIRST%28peggy%29+AND+SUBJAREA%28ARTS%29\nand using headers {'X-ELS-APIKey': '6af3d2c09eb08ec6e12f0cead9a1f5bb', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"INVALID_INPUT\",\"statusText\":\"Error translating query\"}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-4ef4e358d7ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mauth_srch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElsSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mauth_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elssearch.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, els_client, get_all)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m## TODO: add exception handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mapi_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mels_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tot_num_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'opensearch:totalResults'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elsclient.py\u001b[0m in \u001b[0;36mexec_request\u001b[1;34m(self, URL)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\nand using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP 400 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28or%29+AND+AUTHFIRST%28peggy%29+AND+SUBJAREA%28ARTS%29\nand using headers {'X-ELS-APIKey': '2dab4694b4347fa574d159bb97484fc4', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"INVALID_INPUT\",\"statusText\":\"Error translating query\"}}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-4ef4e358d7ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mauth_srch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElsSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mauth_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elssearch.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, els_client, get_all)\u001b[0m\n\u001b[0;32m     93\u001b[0m             all results for the search, up to a maximum of 5,000.\"\"\"\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m## TODO: add exception handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mapi_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mels_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tot_num_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'opensearch:totalResults'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entry'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\elsapy\\elsclient.py\u001b[0m in \u001b[0;36mexec_request\u001b[1;34m(self, URL)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HTTP \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error from \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\nand using headers \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP 400 Error from https://api.elsevier.com/content/search/author?query=AUTHLAST%28or%29+AND+AUTHFIRST%28peggy%29+AND+SUBJAREA%28ARTS%29\nand using headers {'X-ELS-APIKey': '6af3d2c09eb08ec6e12f0cead9a1f5bb', 'User-Agent': 'elsapy-v0.5.0', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"INVALID_INPUT\",\"statusText\":\"Error translating query\"}}}"
     ]
    }
   ],
   "source": [
    "# Full Loop:\n",
    "# Fixed API keys lists\n",
    "\n",
    "# new_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "used_api_keys_list = list(\"8707db153e4b9672fa6df25b03a5f747\")\n",
    "\n",
    "client = ElsClient(\"b7e95f5ea731eb5c9e84e7a1d499f50e\",\"30195f0b2192052a36bcab9ce3c4064f\")\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in new_author_input_dict:\n",
    "    given_in = new_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = new_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = new_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "    except:\n",
    "        if len(avail_api_keys_list) >0:\n",
    "            with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(new_author_out_list, p)\n",
    "            print(\"Loop save complete!\")\n",
    "            new_key = avail_api_keys_list.pop(0)            \n",
    "            client = ElsClient(new_key)\n",
    "            \n",
    "            print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "            \n",
    "            print(f\"Changed API key, now using key: {new_key}\")\n",
    "            used_api_keys_list.append(new_key)\n",
    "\n",
    "            auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "            auth_srch.execute(client)\n",
    "\n",
    "        else:\n",
    "            print(\"No available API keys to use!\")\n",
    "            break\n",
    "\n",
    "\n",
    "    country_out = list()\n",
    "    given_out = list()\n",
    "    family_out = list()\n",
    "    doc_count_out = list()\n",
    "\n",
    "    for auth in auth_srch.results:\n",
    "        # Country:\n",
    "        try:\n",
    "            country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Given:\n",
    "        try:\n",
    "            given_out.append(auth['preferred-name']['given-name'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Family:\n",
    "        try:\n",
    "            family_out.append(auth['preferred-name']['surname'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Doc Count:\n",
    "        try:\n",
    "            doc_count_out.append(auth['document-count'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "\n",
    "    new_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Improved Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_author_out_list\",\"wb\") as p:\n",
    "    pickle.dump(new_author_out_list, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>country</th>\n",
       "      <th>given</th>\n",
       "      <th>family</th>\n",
       "      <th>doc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aut_0</td>\n",
       "      <td>[United States, China, India, Saudi Arabia, Om...</td>\n",
       "      <td>[Md Zakirul Alam, Md Mahbub, Moshahid Alam, Pr...</td>\n",
       "      <td>[Bhuiyan, Alam, Rizvi, Alam, Khan, Sarker, Ala...</td>\n",
       "      <td>[230, 209, 159, 113, 111, 105, 82, 82, 74, 68,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aut_1</td>\n",
       "      <td>[Spain, Portugal]</td>\n",
       "      <td>[Antonio, Maria Alcina]</td>\n",
       "      <td>[Alcina, Pereira]</td>\n",
       "      <td>[84, 84]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aut_2</td>\n",
       "      <td>[Jordan]</td>\n",
       "      <td>[Abeer]</td>\n",
       "      <td>[AlHadidi]</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aut_3</td>\n",
       "      <td>[United States]</td>\n",
       "      <td>[Andriy, D. A.]</td>\n",
       "      <td>[Andreyev, Andreyev]</td>\n",
       "      <td>[27, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aut_4</td>\n",
       "      <td>[Saudi Arabia]</td>\n",
       "      <td>[Abdulkadir Sh]</td>\n",
       "      <td>[Aydarous]</td>\n",
       "      <td>[27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24258</th>\n",
       "      <td>aut_24261</td>\n",
       "      <td>[Poland]</td>\n",
       "      <td>[Łukasz]</td>\n",
       "      <td>[Markiewicz]</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24259</th>\n",
       "      <td>aut_24262</td>\n",
       "      <td>[Poland]</td>\n",
       "      <td>[Łukasz]</td>\n",
       "      <td>[Rudnicki]</td>\n",
       "      <td>[52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24260</th>\n",
       "      <td>aut_24263</td>\n",
       "      <td>[Czech Republic]</td>\n",
       "      <td>[Šárka]</td>\n",
       "      <td>[Techlovská]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24261</th>\n",
       "      <td>aut_24264</td>\n",
       "      <td>[Slovenia]</td>\n",
       "      <td>[Špela]</td>\n",
       "      <td>[Tajnšek]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24262</th>\n",
       "      <td>aut_24265</td>\n",
       "      <td>[Slovenia]</td>\n",
       "      <td>[Štefan]</td>\n",
       "      <td>[Bojnec]</td>\n",
       "      <td>[192]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24263 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                            country  \\\n",
       "0          aut_0  [United States, China, India, Saudi Arabia, Om...   \n",
       "1          aut_1                                  [Spain, Portugal]   \n",
       "2          aut_2                                           [Jordan]   \n",
       "3          aut_3                                    [United States]   \n",
       "4          aut_4                                     [Saudi Arabia]   \n",
       "...          ...                                                ...   \n",
       "24258  aut_24261                                           [Poland]   \n",
       "24259  aut_24262                                           [Poland]   \n",
       "24260  aut_24263                                   [Czech Republic]   \n",
       "24261  aut_24264                                         [Slovenia]   \n",
       "24262  aut_24265                                         [Slovenia]   \n",
       "\n",
       "                                                   given  \\\n",
       "0      [Md Zakirul Alam, Md Mahbub, Moshahid Alam, Pr...   \n",
       "1                                [Antonio, Maria Alcina]   \n",
       "2                                                [Abeer]   \n",
       "3                                        [Andriy, D. A.]   \n",
       "4                                        [Abdulkadir Sh]   \n",
       "...                                                  ...   \n",
       "24258                                           [Łukasz]   \n",
       "24259                                           [Łukasz]   \n",
       "24260                                            [Šárka]   \n",
       "24261                                            [Špela]   \n",
       "24262                                           [Štefan]   \n",
       "\n",
       "                                                  family  \\\n",
       "0      [Bhuiyan, Alam, Rizvi, Alam, Khan, Sarker, Ala...   \n",
       "1                                      [Alcina, Pereira]   \n",
       "2                                             [AlHadidi]   \n",
       "3                                   [Andreyev, Andreyev]   \n",
       "4                                             [Aydarous]   \n",
       "...                                                  ...   \n",
       "24258                                       [Markiewicz]   \n",
       "24259                                         [Rudnicki]   \n",
       "24260                                       [Techlovská]   \n",
       "24261                                          [Tajnšek]   \n",
       "24262                                           [Bojnec]   \n",
       "\n",
       "                                               doc_count  \n",
       "0      [230, 209, 159, 113, 111, 105, 82, 82, 74, 68,...  \n",
       "1                                               [84, 84]  \n",
       "2                                                   [24]  \n",
       "3                                                [27, 1]  \n",
       "4                                                   [27]  \n",
       "...                                                  ...  \n",
       "24258                                               [25]  \n",
       "24259                                               [52]  \n",
       "24260                                                [2]  \n",
       "24261                                                [2]  \n",
       "24262                                              [192]  \n",
       "\n",
       "[24263 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24266"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_first_author_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aut_23772</th>\n",
       "      <td>yujiu</td>\n",
       "      <td>zhao</td>\n",
       "      <td>[Earth and Planetary Sciences, Engineering]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_23773</th>\n",
       "      <td>yujuan</td>\n",
       "      <td>chen</td>\n",
       "      <td>[Mathematics, Economics, Econometrics and Fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_23774</th>\n",
       "      <td>yujuan</td>\n",
       "      <td>li</td>\n",
       "      <td>[Chemical Engineering, Materials Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_23775</th>\n",
       "      <td>yuka</td>\n",
       "      <td>inamochi</td>\n",
       "      <td>[Dentistry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_23776</th>\n",
       "      <td>yuki</td>\n",
       "      <td>adam</td>\n",
       "      <td>[Social Sciences, Medicine, Psychology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_24261</th>\n",
       "      <td>łukasz</td>\n",
       "      <td>markiewicz</td>\n",
       "      <td>[Psychology, Arts and Humanities, Decision Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_24262</th>\n",
       "      <td>łukasz</td>\n",
       "      <td>rudnicki</td>\n",
       "      <td>[Mathematics, Physics and Astronomy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_24263</th>\n",
       "      <td>šarka</td>\n",
       "      <td>techlovska</td>\n",
       "      <td>[Neuroscience, Pharmacology, Toxicology and Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_24264</th>\n",
       "      <td>špela</td>\n",
       "      <td>tajnšek</td>\n",
       "      <td>[Pharmacology, Toxicology and Pharmaceutics, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_24265</th>\n",
       "      <td>štefan</td>\n",
       "      <td>bojnec</td>\n",
       "      <td>[Computer Science, Engineering, Business, Mana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          new_given  new_family  \\\n",
       "index                             \n",
       "aut_23772     yujiu        zhao   \n",
       "aut_23773    yujuan        chen   \n",
       "aut_23774    yujuan          li   \n",
       "aut_23775      yuka    inamochi   \n",
       "aut_23776      yuki        adam   \n",
       "...             ...         ...   \n",
       "aut_24261    łukasz  markiewicz   \n",
       "aut_24262    łukasz    rudnicki   \n",
       "aut_24263     šarka  techlovska   \n",
       "aut_24264     špela     tajnšek   \n",
       "aut_24265    štefan      bojnec   \n",
       "\n",
       "                                                     SA_list  \n",
       "index                                                         \n",
       "aut_23772        [Earth and Planetary Sciences, Engineering]  \n",
       "aut_23773  [Mathematics, Economics, Econometrics and Fina...  \n",
       "aut_23774          [Chemical Engineering, Materials Science]  \n",
       "aut_23775                                        [Dentistry]  \n",
       "aut_23776            [Social Sciences, Medicine, Psychology]  \n",
       "...                                                      ...  \n",
       "aut_24261  [Psychology, Arts and Humanities, Decision Sci...  \n",
       "aut_24262               [Mathematics, Physics and Astronomy]  \n",
       "aut_24263  [Neuroscience, Pharmacology, Toxicology and Ph...  \n",
       "aut_24264  [Pharmacology, Toxicology and Pharmaceutics, M...  \n",
       "aut_24265  [Computer Science, Engineering, Business, Mana...  \n",
       "\n",
       "[494 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]][23772:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_input_dict = new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]][23772:].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 20:41:55\n"
     ]
    }
   ],
   "source": [
    "# Full Loop - Improved:\n",
    "\n",
    "#new_author_out_list= list()\n",
    "\n",
    "# avail_api_keys_list = [\"8707db153e4b9672fa6df25b03a5f747\",\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "# used_api_keys_list = list()\n",
    "\n",
    "# current_key = avail_api_keys_list.pop(0)\n",
    "# client = ElsClient(current_key)\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in new_author_input_dict:\n",
    "    given_in = new_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = new_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = new_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "        cnt_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        err_no = re.search(\"\\d+\",str(e))[0]\n",
    "\n",
    "        print(f\"Encountered Error: {err_no}!\")\n",
    "        print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "        with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(new_author_out_list, p)\n",
    "        print(\"Loop save complete!\")\n",
    "\n",
    "        if \"400\" in str(e):\n",
    "            # query error, need to skip\n",
    "            print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "            cnt_check = False\n",
    "\n",
    "        elif \"401\" in str(e):\n",
    "            # VPN error, sleep try again\n",
    "            sleep(60)\n",
    "            print(\"Slept for 60 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                print(\"Got an error again & breaking the loop ! :(\")\n",
    "                break\n",
    "            \n",
    "        elif \"429\" in str(e):\n",
    "            # change API key \n",
    "            while len(avail_api_keys_list) >0:\n",
    "                used_api_keys_list.append(current_key)\n",
    "                current_key = avail_api_keys_list.pop(0)\n",
    "                client = ElsClient(current_key)\n",
    "                print(f\"Changed API key, now using key: {current_key}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"New key is OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"429\":\n",
    "                        print(\"New key is also finished!\")\n",
    "                    else:\n",
    "                        print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "                        cnt_check = False\n",
    "                        break\n",
    "\n",
    "            if len(avail_api_keys_list) == 0:\n",
    "                print(\"No available API keys left & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"500\" in str(e):\n",
    "            sleep(30)\n",
    "            print(\"Slept for 30 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                cnt_check = False\n",
    "            pass\n",
    "        \n",
    "        elif \"10060\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown error & breaking the loop! :(\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if cnt_check:\n",
    "        country_out = list()\n",
    "        given_out = list()\n",
    "        family_out = list()\n",
    "        doc_count_out = list()\n",
    "\n",
    "        for auth in auth_srch.results:\n",
    "            # Country:\n",
    "            try:\n",
    "                country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Given:\n",
    "            try:\n",
    "                given_out.append(auth['preferred-name']['given-name'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Family:\n",
    "            try:\n",
    "                family_out.append(auth['preferred-name']['surname'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Doc Count:\n",
    "            try:\n",
    "                doc_count_out.append(auth['document-count'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "        new_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOP FINISHED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "    pickle.dump(new_author_out_list, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: k5k datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7e_variables/last_5k_run_Sum/k5k_new_succ\", \"rb\") as fp:\n",
    "    k5k_new_succ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5k_author_df = k5k_new_succ.dropna(subset= \"author\").reset_index().loc[:,[\"DOI\", \"JRNL_ID\", \"author\"]].explode(\"author\").reset_index(drop=True)\n",
    "\n",
    "k5k_aut_df = pd.DataFrame(list(k5k_author_df['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the two datasets:\n",
    "k5k_author_df = pd.concat([k5k_author_df, k5k_aut_df], axis=1)\n",
    "\n",
    "k5k_author_df.drop([\"suffix\", \"name\", \"author\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty given & family:\n",
    "k5k_author_df.dropna(subset=[\"family\", \"given\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to df[0] = df[0].str.strip() first:\n",
    "k5k_author_df.given = k5k_author_df.given.str.strip()\n",
    "k5k_author_df.family = k5k_author_df.family.str.strip()\n",
    "\n",
    "# Remove multiple spaces:\n",
    "k5k_author_df.given = k5k_author_df.given.map(lambda x: re.sub(' +', ' ', x))\n",
    "k5k_author_df.family = k5k_author_df.family.map(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "\n",
    "# Case 1: A. C\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x))),\"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x)))].given.map(lambda x: x +\".\")\n",
    "# Case 2: A.C\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x))),\"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x)))].given.map(lambda x: x.replace(\".\",\". \") +\".\")\n",
    "# Case 3: AC\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x))),\"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x)))].given.map(lambda x: x[0] + \". \" + x[1] + \".\")\n",
    "# Case 4: A C\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x))),\"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x)))].given.map(lambda x: x.replace(\" \", \". \") + \".\")\n",
    "# Case 5: A.C.\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x))), \"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x)))].given.map(lambda x: x.replace(\".\", \". \").strip())\n",
    "\n",
    "# Single letter names:\n",
    "k5k_author_df.loc[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x))),\"new_given\"] = k5k_author_df[k5k_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x)))].given.map(lambda x: x + \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpli:\n",
    "\n",
    "k5k_author_df.loc[:,\"new_given\"] = k5k_author_df.given.map(name_simplifier)\n",
    "k5k_author_df.loc[:,\"new_family\"] = k5k_author_df.family.map(name_simplifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After process stripping:\n",
    "\n",
    "k5k_author_df.given = k5k_author_df.given.str.strip()\n",
    "k5k_author_df.family = k5k_author_df.family.str.strip()\n",
    "\n",
    "k5k_author_df.new_given = k5k_author_df.new_given.str.strip()\n",
    "k5k_author_df.new_family = k5k_author_df.new_family.str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)\n",
    "\n",
    "k5k_author_df = pd.merge(left=k5k_author_df, right=q1_jrnl_df[\"SA_list\"], how=\"left\", left_on=\"JRNL_ID\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a first author df\n",
    "# Using .groupby() to have DOI & SA_list columns \n",
    "\n",
    "\n",
    "k5k_first_author_df = k5k_author_df[k5k_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"SA_list\"].apply(list)\n",
    "k5k_first_author_df = k5k_first_author_df.to_frame().merge(k5k_author_df[k5k_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"DOI\"].apply(list), how= \"inner\", right_index=True, left_index=True)\n",
    "k5k_first_author_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [new_given, new_family, SA_list, DOI]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k5k_first_author_df[k5k_first_author_df.new_given.map(lambda x: bool(re.match(\"\\W\",x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [new_given, new_family, SA_list, DOI]\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k5k_first_author_df[k5k_first_author_df.new_family.map(lambda x: bool(re.match(\"\\W\",x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Flattening SA_list col:\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "k5k_first_author_df.loc[:,\"SA_list\"] = k5k_first_author_df.SA_list.map(flatten)\n",
    "\n",
    "# 2- Creating an Author ID col:\n",
    "k5k_first_author_df.reset_index(drop=True, inplace=True)\n",
    "k5k_first_author_df.reset_index(drop=False,inplace=True)\n",
    "k5k_first_author_df.loc[:,\"index\"] = k5k_first_author_df.index.map(lambda x: \"aut_\" + str(x))\n",
    "k5k_first_author_df.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"k5k_first_author_df\",\"wb\") as p:\n",
    "    pickle.dump(k5k_first_author_df, p)\n",
    "\n",
    "with open(\"k5k_author_df\",\"wb\") as p:\n",
    "    pickle.dump(k5k_author_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: k5k API Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5k_author_input_dict = k5k_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 23:29:00\n",
      "Encountered Error: 429!\n",
      "Time = 23:29:02\n",
      "Loop save complete!\n",
      "Changed API key, now using key: 2dab4694b4347fa574d159bb97484fc4\n",
      "New key is also finished!\n",
      "Changed API key, now using key: 6af3d2c09eb08ec6e12f0cead9a1f5bb\n",
      "New key is OK!\n",
      "Encountered Error: 429!\n",
      "Time = 23:48:58\n",
      "Loop save complete!\n",
      "Changed API key, now using key: 3872e798bf48fa28af583b9ebef5deb6\n",
      "New key is OK!\n",
      "Encountered Error: 429!\n",
      "Time = 03:07:30\n",
      "Loop save complete!\n",
      "Changed API key, now using key: 1473c31dbcdb425f9cdaf75c673279d3\n",
      "New key is OK!\n",
      "No available API keys left & breaking the loop! :(\n"
     ]
    }
   ],
   "source": [
    "# Full Loop - Improved:\n",
    "\n",
    "k5k_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"8707db153e4b9672fa6df25b03a5f747\",\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "used_api_keys_list = list()\n",
    "\n",
    "current_key = avail_api_keys_list.pop(0)\n",
    "client = ElsClient(current_key)\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in k5k_author_input_dict:\n",
    "    given_in = k5k_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = k5k_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = k5k_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "        cnt_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        err_no = re.search(\"\\d+\",str(e))[0]\n",
    "\n",
    "        print(f\"Encountered Error: {err_no}!\")\n",
    "        print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "        with open(\"k5k_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(k5k_author_out_list, p)\n",
    "        print(\"Loop save complete!\")\n",
    "\n",
    "        if \"400\" in str(e):\n",
    "            # query error, need to skip\n",
    "            print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "            cnt_check = False\n",
    "\n",
    "        elif \"401\" in str(e):\n",
    "            # VPN error, sleep try again\n",
    "            sleep(60)\n",
    "            print(\"Slept for 60 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                print(\"Got an error again & breaking the loop ! :(\")\n",
    "                break\n",
    "            \n",
    "        elif \"429\" in str(e):\n",
    "            # change API key \n",
    "            while len(avail_api_keys_list) >0:\n",
    "                used_api_keys_list.append(current_key)\n",
    "                current_key = avail_api_keys_list.pop(0)\n",
    "                client = ElsClient(current_key)\n",
    "                print(f\"Changed API key, now using key: {current_key}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"New key is OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"429\":\n",
    "                        print(\"New key is also finished!\")\n",
    "                    else:\n",
    "                        print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "                        cnt_check = False\n",
    "                        break\n",
    "\n",
    "            if len(avail_api_keys_list) == 0:\n",
    "                print(\"No available API keys left & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"500\" in str(e):\n",
    "            sleep(30)\n",
    "            print(\"Slept for 30 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                cnt_check = False\n",
    "            pass\n",
    "        \n",
    "        elif \"10060\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown error & breaking the loop! :(\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if cnt_check:\n",
    "        country_out = list()\n",
    "        given_out = list()\n",
    "        family_out = list()\n",
    "        doc_count_out = list()\n",
    "\n",
    "        for auth in auth_srch.results:\n",
    "            # Country:\n",
    "            try:\n",
    "                country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Given:\n",
    "            try:\n",
    "                given_out.append(auth['preferred-name']['given-name'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Family:\n",
    "            try:\n",
    "                family_out.append(auth['preferred-name']['surname'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Doc Count:\n",
    "            try:\n",
    "                doc_count_out.append(auth['document-count'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "        k5k_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_given</th>\n",
       "      <th>new_family</th>\n",
       "      <th>SA_list</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aut_0</th>\n",
       "      <td>a</td>\n",
       "      <td>alqerban</td>\n",
       "      <td>[Dentistry, Medicine]</td>\n",
       "      <td>[10.1259/dmfr.20130157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_1</th>\n",
       "      <td>a</td>\n",
       "      <td>cassioli</td>\n",
       "      <td>[Health Professions, Medicine]</td>\n",
       "      <td>[10.1088/0031-9155/58/2/301]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_2</th>\n",
       "      <td>a</td>\n",
       "      <td>dietrich</td>\n",
       "      <td>[Medicine, Nursing]</td>\n",
       "      <td>[10.1038/ijo.2016.28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_3</th>\n",
       "      <td>a</td>\n",
       "      <td>drewnowski</td>\n",
       "      <td>[Medicine, Nursing]</td>\n",
       "      <td>[10.1038/ijo.2013.179]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_4</th>\n",
       "      <td>a</td>\n",
       "      <td>finlayson</td>\n",
       "      <td>[Business, Management and Accounting, Decision...</td>\n",
       "      <td>[10.1057/jors.2012.127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_5587</th>\n",
       "      <td>zorana</td>\n",
       "      <td>jančić</td>\n",
       "      <td>[Computer Science, Engineering, Decision Scien...</td>\n",
       "      <td>[10.1016/j.ins.2010.12.008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_5588</th>\n",
       "      <td>zoë</td>\n",
       "      <td>avner</td>\n",
       "      <td>[Social Sciences, Medicine, Health Professions]</td>\n",
       "      <td>[10.1080/13573322.2020.1782881]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_5589</th>\n",
       "      <td>zs.</td>\n",
       "      <td>szendro</td>\n",
       "      <td>[Agricultural and Biological Sciences, Veterin...</td>\n",
       "      <td>[10.1016/j.livsci.2010.11.012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_5590</th>\n",
       "      <td>zucheng</td>\n",
       "      <td>luo</td>\n",
       "      <td>[Biochemistry, Genetics and Molecular Biology,...</td>\n",
       "      <td>[10.1038/s41419-019-1410-y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aut_5591</th>\n",
       "      <td>łukasz</td>\n",
       "      <td>lenart</td>\n",
       "      <td>[Mathematics, Decision Sciences]</td>\n",
       "      <td>[10.1111/jtsa.12163]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5592 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         new_given  new_family  \\\n",
       "index                            \n",
       "aut_0            a    alqerban   \n",
       "aut_1            a    cassioli   \n",
       "aut_2            a    dietrich   \n",
       "aut_3            a  drewnowski   \n",
       "aut_4            a   finlayson   \n",
       "...            ...         ...   \n",
       "aut_5587    zorana      jančić   \n",
       "aut_5588       zoë       avner   \n",
       "aut_5589       zs.     szendro   \n",
       "aut_5590   zucheng         luo   \n",
       "aut_5591    łukasz      lenart   \n",
       "\n",
       "                                                    SA_list  \\\n",
       "index                                                         \n",
       "aut_0                                 [Dentistry, Medicine]   \n",
       "aut_1                        [Health Professions, Medicine]   \n",
       "aut_2                                   [Medicine, Nursing]   \n",
       "aut_3                                   [Medicine, Nursing]   \n",
       "aut_4     [Business, Management and Accounting, Decision...   \n",
       "...                                                     ...   \n",
       "aut_5587  [Computer Science, Engineering, Decision Scien...   \n",
       "aut_5588    [Social Sciences, Medicine, Health Professions]   \n",
       "aut_5589  [Agricultural and Biological Sciences, Veterin...   \n",
       "aut_5590  [Biochemistry, Genetics and Molecular Biology,...   \n",
       "aut_5591                   [Mathematics, Decision Sciences]   \n",
       "\n",
       "                                      DOI  \n",
       "index                                      \n",
       "aut_0             [10.1259/dmfr.20130157]  \n",
       "aut_1        [10.1088/0031-9155/58/2/301]  \n",
       "aut_2               [10.1038/ijo.2016.28]  \n",
       "aut_3              [10.1038/ijo.2013.179]  \n",
       "aut_4             [10.1057/jors.2012.127]  \n",
       "...                                   ...  \n",
       "aut_5587      [10.1016/j.ins.2010.12.008]  \n",
       "aut_5588  [10.1080/13573322.2020.1782881]  \n",
       "aut_5589   [10.1016/j.livsci.2010.11.012]  \n",
       "aut_5590      [10.1038/s41419-019-1410-y]  \n",
       "aut_5591             [10.1111/jtsa.12163]  \n",
       "\n",
       "[5592 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k5k_first_author_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"k5k_author_out_list\",\"wb\") as p:\n",
    "    pickle.dump(k5k_author_out_list, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 300 records left in this loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5k_author_input_dict = k5k_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]][5202:].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 08:30:28\n"
     ]
    }
   ],
   "source": [
    "# Full Loop - Improved:\n",
    "\n",
    "# k5k_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"b7e95f5ea731eb5c9e84e7a1d499f50e\", \"30195f0b2192052a36bcab9ce3c4064f\"]\n",
    "\n",
    "# used_api_keys_list = list()\n",
    "\n",
    "current_key = avail_api_keys_list.pop(0)\n",
    "client = ElsClient(current_key)\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in k5k_author_input_dict:\n",
    "    given_in = k5k_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = k5k_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = k5k_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "        cnt_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        err_no = re.search(\"\\d+\",str(e))[0]\n",
    "\n",
    "        print(f\"Encountered Error: {err_no}!\")\n",
    "        print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "        with open(\"k5k_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(k5k_author_out_list, p)\n",
    "        print(\"Loop save complete!\")\n",
    "\n",
    "        if \"400\" in str(e):\n",
    "            # query error, need to skip\n",
    "            print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "            cnt_check = False\n",
    "\n",
    "        elif \"401\" in str(e):\n",
    "            # VPN error, sleep try again\n",
    "            sleep(60)\n",
    "            print(\"Slept for 60 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                print(\"Got an error again & breaking the loop ! :(\")\n",
    "                break\n",
    "            \n",
    "        elif \"429\" in str(e):\n",
    "            # change API key \n",
    "            while len(avail_api_keys_list) > 0:\n",
    "                used_api_keys_list.append(current_key)\n",
    "                current_key = avail_api_keys_list.pop(0)\n",
    "                client = ElsClient(current_key)\n",
    "                print(f\"Changed API key, now using key: {current_key}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"New key is OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"429\":\n",
    "                        print(\"New key is also finished!\")\n",
    "                    else:\n",
    "                        print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "                        cnt_check = False\n",
    "                        break\n",
    "\n",
    "            if len(avail_api_keys_list) == 0:\n",
    "                print(\"No available API keys left & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"500\" in str(e):\n",
    "            sleep(30)\n",
    "            print(\"Slept for 30 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                cnt_check = False\n",
    "            pass\n",
    "        \n",
    "        elif \"10060\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown error & breaking the loop! :(\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if cnt_check:\n",
    "        country_out = list()\n",
    "        given_out = list()\n",
    "        family_out = list()\n",
    "        doc_count_out = list()\n",
    "\n",
    "        for auth in auth_srch.results:\n",
    "            # Country:\n",
    "            try:\n",
    "                country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Given:\n",
    "            try:\n",
    "                given_out.append(auth['preferred-name']['given-name'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Family:\n",
    "            try:\n",
    "                family_out.append(auth['preferred-name']['surname'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Doc Count:\n",
    "            try:\n",
    "                doc_count_out.append(auth['document-count'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "        k5k_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>country</th>\n",
       "      <th>given</th>\n",
       "      <th>family</th>\n",
       "      <th>doc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aut_0</td>\n",
       "      <td>[Saudi Arabia]</td>\n",
       "      <td>[Ali]</td>\n",
       "      <td>[Alqerban]</td>\n",
       "      <td>[48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aut_1</td>\n",
       "      <td>[Denmark]</td>\n",
       "      <td>[Andrea]</td>\n",
       "      <td>[Cassioli]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aut_2</td>\n",
       "      <td>[United States, United States, Austria, German...</td>\n",
       "      <td>[Andrea M., Allen J., Dietrich, Arne, Alexande...</td>\n",
       "      <td>[Dietrich, Dietrich, Albert, Dietrich, Dietric...</td>\n",
       "      <td>[171, 158, 148, 131, 112, 69, 30, 22, 19, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aut_3</td>\n",
       "      <td>[United States, United States]</td>\n",
       "      <td>[Adam, Adam]</td>\n",
       "      <td>[Drewnowski, Drewnowski]</td>\n",
       "      <td>[455, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aut_4</td>\n",
       "      <td>[United Kingdom]</td>\n",
       "      <td>[A.]</td>\n",
       "      <td>[Finlayson]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>aut_5587</td>\n",
       "      <td>[Serbia]</td>\n",
       "      <td>[Zorana]</td>\n",
       "      <td>[Jančić]</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5588</th>\n",
       "      <td>aut_5588</td>\n",
       "      <td>[United Kingdom]</td>\n",
       "      <td>[Zoë]</td>\n",
       "      <td>[Avner]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5589</th>\n",
       "      <td>aut_5589</td>\n",
       "      <td>[Hungary]</td>\n",
       "      <td>[Zs]</td>\n",
       "      <td>[Szendrő]</td>\n",
       "      <td>[135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5590</th>\n",
       "      <td>aut_5590</td>\n",
       "      <td>[China]</td>\n",
       "      <td>[Zucheng]</td>\n",
       "      <td>[Luo]</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591</th>\n",
       "      <td>aut_5591</td>\n",
       "      <td>[Poland]</td>\n",
       "      <td>[Łukasz]</td>\n",
       "      <td>[Lenart]</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5592 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_id                                            country  \\\n",
       "0        aut_0                                     [Saudi Arabia]   \n",
       "1        aut_1                                          [Denmark]   \n",
       "2        aut_2  [United States, United States, Austria, German...   \n",
       "3        aut_3                     [United States, United States]   \n",
       "4        aut_4                                   [United Kingdom]   \n",
       "...        ...                                                ...   \n",
       "5587  aut_5587                                           [Serbia]   \n",
       "5588  aut_5588                                   [United Kingdom]   \n",
       "5589  aut_5589                                          [Hungary]   \n",
       "5590  aut_5590                                            [China]   \n",
       "5591  aut_5591                                           [Poland]   \n",
       "\n",
       "                                                  given  \\\n",
       "0                                                 [Ali]   \n",
       "1                                              [Andrea]   \n",
       "2     [Andrea M., Allen J., Dietrich, Arne, Alexande...   \n",
       "3                                          [Adam, Adam]   \n",
       "4                                                  [A.]   \n",
       "...                                                 ...   \n",
       "5587                                           [Zorana]   \n",
       "5588                                              [Zoë]   \n",
       "5589                                               [Zs]   \n",
       "5590                                          [Zucheng]   \n",
       "5591                                           [Łukasz]   \n",
       "\n",
       "                                                 family  \\\n",
       "0                                            [Alqerban]   \n",
       "1                                            [Cassioli]   \n",
       "2     [Dietrich, Dietrich, Albert, Dietrich, Dietric...   \n",
       "3                              [Drewnowski, Drewnowski]   \n",
       "4                                           [Finlayson]   \n",
       "...                                                 ...   \n",
       "5587                                           [Jančić]   \n",
       "5588                                            [Avner]   \n",
       "5589                                          [Szendrő]   \n",
       "5590                                              [Luo]   \n",
       "5591                                           [Lenart]   \n",
       "\n",
       "                                              doc_count  \n",
       "0                                                  [48]  \n",
       "1                                                  [17]  \n",
       "2     [171, 158, 148, 131, 112, 69, 30, 22, 19, 11, ...  \n",
       "3                                              [455, 1]  \n",
       "4                                                   [1]  \n",
       "...                                                 ...  \n",
       "5587                                                [9]  \n",
       "5588                                                [7]  \n",
       "5589                                              [135]  \n",
       "5590                                               [12]  \n",
       "5591                                               [14]  \n",
       "\n",
       "[5592 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(k5k_author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"k5k_author_out_list_loop_save\",\"wb\") as p:\n",
    "    pickle.dump(k5k_author_out_list, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Combining author_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading variables:\n",
    "\n",
    "# Main:\n",
    "with open(\"q1_author_df\", \"rb\") as fp:\n",
    "    q1_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"q1_first_author_df\", \"rb\") as fp:\n",
    "    q1_first_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"author_out_list\", \"rb\") as fp:\n",
    "    author_out_list = pickle.load(fp)\n",
    "\n",
    "\n",
    "# New (20K):\n",
    "with open(\"7d_variables/7d2_variables/new_author_df\", \"rb\") as fp:\n",
    "    new_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"7d_variables/7d2_variables/new_first_author_df\", \"rb\") as fp:\n",
    "    new_first_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"7d_variables/7d2_variables/new_author_out_list\", \"rb\") as fp:\n",
    "    new_author_out_list = pickle.load(fp)\n",
    "\n",
    "\n",
    "# k5k (5k):\n",
    "with open(\"7d_variables/7d2_variables/k5k_author_df\", \"rb\") as fp:\n",
    "    k5k_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"7d_variables/7d2_variables/k5k_first_author_df\", \"rb\") as fp:\n",
    "    k5k_first_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"7d_variables/7d2_variables/k5k_author_out_list\", \"rb\") as fp:\n",
    "    k5k_author_out_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_first_author_df = q1_first_author_df.merge(pd.DataFrame(author_out_list).set_index(\"author_id\"), how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_author_df = new_first_author_df.merge(pd.DataFrame(new_author_out_list).set_index(\"author_id\"), how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5k_first_author_df = k5k_first_author_df.merge(pd.DataFrame(k5k_author_out_list).set_index(\"author_id\"), how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_first_author_df = pd.concat([q1_first_author_df,new_first_author_df,k5k_first_author_df], ignore_index=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_first_author_df.country.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6618"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_first_author_df[all_first_author_df.country.notna()].country.map(len) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>country</th>\n",
       "      <th>given</th>\n",
       "      <th>family</th>\n",
       "      <th>doc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aut_0</td>\n",
       "      <td>[China, China, China, Hong Kong]</td>\n",
       "      <td>[Jing, Jingheng, Jing, (Luna) Jing]</td>\n",
       "      <td>[Cai, Cai, Cai, Cai]</td>\n",
       "      <td>[23, 22, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aut_1</td>\n",
       "      <td>[United States, United States, United States, ...</td>\n",
       "      <td>[Qing, Qing, (Grace) Qing, Grace Qing]</td>\n",
       "      <td>[Hao, Hao, Hao, Hao]</td>\n",
       "      <td>[86, 11, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aut_2</td>\n",
       "      <td>[China, China, China, China, China, China, Chi...</td>\n",
       "      <td>[Yongquan, Yonghong, Yong, Dayong, Yongwu, Yon...</td>\n",
       "      <td>[Zhou, Zhou, Zhou, Zhou, Zhou, Zhou, Zhou, Zho...</td>\n",
       "      <td>[286, 220, 193, 170, 164, 151, 100, 92, 74, 64...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aut_3</td>\n",
       "      <td>[Italy, Italy, France, Nigeria, Malaysia, Saud...</td>\n",
       "      <td>[Andrea, Alessandro, Ado Adamou Abba, Hafizull...</td>\n",
       "      <td>[Abba, Abbà, Ari, Ahmed, Haruna, Abba, Abba, A...</td>\n",
       "      <td>[75, 65, 44, 19, 17, 15, 8, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aut_4</td>\n",
       "      <td>[Italy]</td>\n",
       "      <td>[Antonella]</td>\n",
       "      <td>[Agodi]</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146526</th>\n",
       "      <td>aut_146533</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146527</th>\n",
       "      <td>aut_146534</td>\n",
       "      <td>[Slovenia]</td>\n",
       "      <td>[Živa Bricman]</td>\n",
       "      <td>[Rejc]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146528</th>\n",
       "      <td>aut_146535</td>\n",
       "      <td>[Uzbekistan]</td>\n",
       "      <td>[A. Kh]</td>\n",
       "      <td>[Inoyatov]</td>\n",
       "      <td>[47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146529</th>\n",
       "      <td>aut_146536</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146530</th>\n",
       "      <td>aut_146537</td>\n",
       "      <td>[Poland]</td>\n",
       "      <td>[Łukasz]</td>\n",
       "      <td>[Boguszewicz]</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146531 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id                                            country  \\\n",
       "0            aut_0                   [China, China, China, Hong Kong]   \n",
       "1            aut_1  [United States, United States, United States, ...   \n",
       "2            aut_2  [China, China, China, China, China, China, Chi...   \n",
       "3            aut_3  [Italy, Italy, France, Nigeria, Malaysia, Saud...   \n",
       "4            aut_4                                            [Italy]   \n",
       "...            ...                                                ...   \n",
       "146526  aut_146533                                                 []   \n",
       "146527  aut_146534                                         [Slovenia]   \n",
       "146528  aut_146535                                       [Uzbekistan]   \n",
       "146529  aut_146536                                                 []   \n",
       "146530  aut_146537                                           [Poland]   \n",
       "\n",
       "                                                    given  \\\n",
       "0                     [Jing, Jingheng, Jing, (Luna) Jing]   \n",
       "1                  [Qing, Qing, (Grace) Qing, Grace Qing]   \n",
       "2       [Yongquan, Yonghong, Yong, Dayong, Yongwu, Yon...   \n",
       "3       [Andrea, Alessandro, Ado Adamou Abba, Hafizull...   \n",
       "4                                             [Antonella]   \n",
       "...                                                   ...   \n",
       "146526                                                 []   \n",
       "146527                                     [Živa Bricman]   \n",
       "146528                                            [A. Kh]   \n",
       "146529                                                 []   \n",
       "146530                                           [Łukasz]   \n",
       "\n",
       "                                                   family  \\\n",
       "0                                    [Cai, Cai, Cai, Cai]   \n",
       "1                                    [Hao, Hao, Hao, Hao]   \n",
       "2       [Zhou, Zhou, Zhou, Zhou, Zhou, Zhou, Zhou, Zho...   \n",
       "3       [Abba, Abbà, Ari, Ahmed, Haruna, Abba, Abba, A...   \n",
       "4                                                 [Agodi]   \n",
       "...                                                   ...   \n",
       "146526                                                 []   \n",
       "146527                                             [Rejc]   \n",
       "146528                                         [Inoyatov]   \n",
       "146529                                                 []   \n",
       "146530                                      [Boguszewicz]   \n",
       "\n",
       "                                                doc_count  \n",
       "0                                          [23, 22, 1, 1]  \n",
       "1                                          [86, 11, 2, 1]  \n",
       "2       [286, 220, 193, 170, 164, 151, 100, 92, 74, 64...  \n",
       "3                          [75, 65, 44, 19, 17, 15, 8, 3]  \n",
       "4                                                   [234]  \n",
       "...                                                   ...  \n",
       "146526                                                 []  \n",
       "146527                                                [7]  \n",
       "146528                                               [47]  \n",
       "146529                                                 []  \n",
       "146530                                               [24]  \n",
       "\n",
       "[146531 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_author_df = pd.concat([q1_author_df,new_author_df,k5k_author_df], ignore_index=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_author_df\",\"wb\") as p:\n",
    "    pickle.dump(all_author_df, p)\n",
    "\n",
    "with open(\"q1_first_author_df\",\"wb\") as p:\n",
    "    pickle.dump(all_first_author_df, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_author_df_copy\",\"wb\") as p:\n",
    "    pickle.dump(all_author_df, p)\n",
    "\n",
    "with open(\"q1_first_author_df_copy\",\"wb\") as p:\n",
    "    pickle.dump(all_first_author_df, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This nb is finished! All necessary function are copied to 7f & all necessary variables are SAVED & DUMPED!\n",
    "\n",
    "I'd like to thank this nb for its amazing service!\n",
    "\n",
    "God speed nb!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d6808e9ba8815475743150367720cad3673ac2b3f4957dc753295ba7ac37a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
