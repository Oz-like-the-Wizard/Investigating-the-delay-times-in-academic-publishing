{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# from crossref.restful import Works, Etiquette\n",
    "# from scihub_blastoise import SciHub_HydroPump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Partial Run SumCetCet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're back after Bar√ßa!\n",
    "\n",
    "- The first order of business is to combine all partial fill SumCetCet run data to a single dataset to analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum:\n",
    "\n",
    "with open(\"sum_pr_cr_results\", \"rb\") as fp:\n",
    "    sum_pr_cr_results = pickle.load(fp)\n",
    "\n",
    "with open(\"sum_pr_sh_succ\", \"rb\") as fp:\n",
    "    sum_pr_sh_succ = pickle.load(fp)\n",
    "\n",
    "with open(\"sum_pr_sh_fail\", \"rb\") as fp:\n",
    "    sum_pr_sh_fail = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cet\n",
    "\n",
    "with open(\"cet_pr_cr_results\", \"rb\") as fp:\n",
    "    cet_pr_cr_results = pickle.load(fp)\n",
    "\n",
    "with open(\"cet_pr_sh_succ\", \"rb\") as fp:\n",
    "    cet_pr_sh_succ = pickle.load(fp)\n",
    "\n",
    "with open(\"cet_pr_sh_fail\", \"rb\") as fp:\n",
    "    cet_pr_sh_fail = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ogi:\n",
    "\n",
    "with open(\"pr_cr_results\", \"rb\") as fp:\n",
    "    ogi_pr_cr_results = pickle.load(fp)\n",
    "\n",
    "with open(\"pr_sh_succ\", \"rb\") as fp:\n",
    "    ogi_pr_sh_succ = pickle.load(fp)\n",
    "\n",
    "with open(\"pr_sh_fail\", \"rb\") as fp:\n",
    "    ogi_pr_sh_fail = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial analysis shows that around 50% of the metadata retr. from CR (pr_cr_results), was used for SH date extraction phase.\n",
    "\n",
    "However, 40-50% of these articles actually contaion DATE info, therefore although the Req_Count is more than satisfied in the CR phase, SH results still do not satisfy for all journals.\n",
    "\n",
    "Once the analysis is complete, we can use the pr_cr_results, to filter out the UNTESTED artcls, and pass them through the SH process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pr_cr_results = pd.DataFrame(sum_pr_cr_results)\n",
    "sum_pr_sh_succ = pd.DataFrame(sum_pr_sh_succ)\n",
    "sum_pr_sh_fail = pd.DataFrame(sum_pr_sh_fail)\n",
    "\n",
    "sum_sh_results =  pd.concat([sum_pr_sh_succ, sum_pr_sh_fail])\n",
    "\n",
    "sum_pr_results = pd.merge(sum_pr_cr_results, sum_sh_results, how=\"left\", on=\"DOI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cet_pr_cr_results = pd.DataFrame(cet_pr_cr_results)\n",
    "cet_pr_sh_succ = pd.DataFrame(cet_pr_sh_succ)\n",
    "cet_pr_sh_fail = pd.DataFrame(cet_pr_sh_fail)\n",
    "\n",
    "cet_sh_results =  pd.concat([cet_pr_sh_succ, cet_pr_sh_fail])\n",
    "\n",
    "cet_pr_results = pd.merge(cet_pr_cr_results, cet_sh_results, how=\"left\", on=\"DOI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogi_pr_cr_results = pd.DataFrame(ogi_pr_cr_results)\n",
    "ogi_pr_sh_succ = pd.DataFrame(ogi_pr_sh_succ)\n",
    "ogi_pr_sh_fail = pd.DataFrame(ogi_pr_sh_fail)\n",
    "\n",
    "ogi_sh_results =  pd.concat([ogi_pr_sh_succ, ogi_pr_sh_fail])\n",
    "\n",
    "ogi_pr_results = pd.merge(ogi_pr_cr_results, ogi_sh_results, how=\"left\", on=\"DOI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating all partial run results to a single dataset: (pr_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_results_df = pd.concat([sum_pr_results, cet_pr_results,ogi_pr_results], ignore_index=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'float'>    41840\n",
       "<class 'str'>      35472\n",
       "<class 'list'>     24708\n",
       "Name: Dates, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_results_df.Dates.map(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the partial run results are consolidated:\n",
    "\n",
    "TTD:\n",
    "- Need to filter out the succesfull artcls, pass them through pre-processing, add them to q1_date_df\n",
    "\n",
    "- Need to filter out FAILED, UNTESTED dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: UPDATING q1_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-6f3939fe8a91>:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  q1_date_pr.loc[:,\"published\"] = q1_date_pr.apply(earlier_date_part, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# TTD 1:\n",
    "# Taken from 7c:\n",
    "\n",
    "# 1.3 - Filtering to only successful artcls:\n",
    "q1_date_pr = pr_results_df[pr_results_df.Dates.map(type)==list]\n",
    "q1_date_pr.set_index(\"DOI\", drop=True, inplace=True)\n",
    "\n",
    "# 1.4 - Feature Eng. : \n",
    "def earlier_date_part(row):\n",
    "    try:\n",
    "        pp_year = row[\"published-print\"][\"date-parts\"][0][0]\n",
    "    except TypeError:\n",
    "        pp_year = None\n",
    "\n",
    "    try:   \n",
    "        po_year = row[\"published-online\"][\"date-parts\"][0][0]\n",
    "    except TypeError:\n",
    "        po_year = None\n",
    "\n",
    "    if po_year is None:\n",
    "        return row[\"published-print\"]\n",
    "\n",
    "    elif pp_year is None:\n",
    "        return row[\"published-online\"]\n",
    "\n",
    "    else:\n",
    "        if pp_year<po_year:\n",
    "            return row[\"published-print\"]\n",
    "        elif po_year<pp_year:\n",
    "            return row[\"published-online\"]\n",
    "        else:\n",
    "            try:\n",
    "                pp_month = row[\"published-print\"][\"date-parts\"][0][1]\n",
    "            except IndexError:\n",
    "                pp_month = 12\n",
    "            try:\n",
    "                po_month = row[\"published-online\"][\"date-parts\"][0][1]\n",
    "            except IndexError:\n",
    "                po_month = 12\n",
    "            if pp_month<po_month:\n",
    "                return row[\"published-print\"]\n",
    "            elif po_month<pp_month:\n",
    "                return row[\"published-online\"]\n",
    "            else:\n",
    "                return row[\"published-online\"]\n",
    "\n",
    "\n",
    "q1_date_pr.loc[:,\"published\"] = q1_date_pr.apply(earlier_date_part, axis=1)\n",
    "\n",
    "q1_date_pr = q1_date_pr[[\"JRNL_ID\", \"ISSN\", \"issued\", \"container-title\", \"publisher\", \"title\", \"author\", \"subject\", \"references-count\", \"is-referenced-by-count\", \"published\", \"published-print\", \"published-online\", \"direct_url\", \"Dates\", \"Keywords\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__q1_date_pr__ is the succesful articles from partial run dataset:\n",
    "\n",
    "However, it should be noted that some (also apparent in q1_date_df) only have \"Accepted Date\" and not \"Received Date\"\n",
    "\n",
    "This MUST be dealt with later in this notebook!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_date_df\", \"rb\") as fp:\n",
    "    q1_date_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140993"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q1_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df = pd.concat([q1_date_df, q1_date_pr], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fixing \"only Accepted Date\" issue:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df = q1_date_df[q1_date_df.Dates.map(lambda x: False if x[0]==None else True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING & DUMPing all used datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_date_pr\",\"wb\") as p:\n",
    "    pickle.dump(q1_date_pr, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_results_df has ALL the extracted info from SumCetCet partial run\n",
    "\n",
    "with open(\"pr_results_df\",\"wb\") as p:\n",
    "    pickle.dump(pr_results_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_date_df\",\"wb\") as p:\n",
    "    pickle.dump(q1_date_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently q1_date_df has 156754 articles that are completely ready for analysis.\n",
    "\n",
    "The remaining TDDs are:\n",
    "\n",
    "__1- Article related:__\n",
    "- Consolidate all UNTESTED article metadata -> only available from partial fill run dataset?\n",
    "- Consolidate all FAIED articles to a single dataset -> final extraction, partial fill, q1_date_df_all, OTHERS?\n",
    "\n",
    "__2- Journal related:__\n",
    "- update q1_jrnl_df and q1_jrnl_expladed with the new information\n",
    "- re-calc Req_Count & check if/how many UNTESTED can be used\n",
    "- compare with pivots to check total necessary data\n",
    "\n",
    "__3- Author related:__\n",
    "- Finalize the author_df \n",
    "- Complete ScopusAPI Author Search run\n",
    "\n",
    "__4- ANALYSIS & STATS & VISUALIZE__\n",
    "\n",
    "__5- WRITE REPORT__\n",
    "\n",
    "__6- FIN!__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: UNTESTED AND FAILED Article Datasets\n",
    "\n",
    "The datasets to be considered are:\n",
    "\n",
    "- q1_date_df_all (Backups)\n",
    "- q1_meta_fe\n",
    "- q1_meta_fe_all\n",
    "- date_df_full (5a)\n",
    "- date_df_5b (5b)\n",
    "- date_df_ok (Main Files)\n",
    "- date_df_ok_final (Main Files)\n",
    "- pr_results_df (7e)\n",
    "\n",
    "To prevent to many variables each will be loaded & compared with q1_date_df and saved accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_doi_list = q1_date_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used for testing below:\n",
    "\n",
    "# with open(\"date_df_ok_final\", \"rb\") as fp:\n",
    "#     testo = pickle.load(fp)\n",
    "\n",
    "# temp_remn = testo[~testo.set_index(\"DOI\").index.isin(succ_doi_list)].set_index(\"DOI\")\n",
    "# temp_remn = testo[~testo.index.isin(succ_doi_list)]\n",
    "\n",
    "# remainder_df = pd.concat([remainder_df,q1_date_pr], verify_integrity=True)\n",
    "# remainder_df = pd.concat([remainder_df,temp_remn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results:__\n",
    "\n",
    "- __q1_date_df_all (Backups)__\n",
    "    - Total len: 36K\n",
    "    - Not in q1_date_df: 4242\n",
    "    - \n",
    "\n",
    "- __q1_meta_fe__\n",
    "    - Total len: 138,184\n",
    "    - Not in q1_date_df: 35,258\n",
    "    - Already included : 131\n",
    "    - \n",
    "\n",
    "- __q1_meta_fe_all__\n",
    "    - Total len: 138,184\n",
    "    - Not in q1_date_df: 35,258 \n",
    "    - SKIPPED AS same as the previous df\n",
    "    - \n",
    "- __date_df_full (5a)__\n",
    "    - Total len: 128,374\n",
    "    - Not in q1_date_df: 96,156\n",
    "    - Already included: 4615\n",
    "    - \n",
    "- __date_df_5b (5b)__\n",
    "    - SKIPPED as pnly 36,514 articles with only SciHub information\n",
    "    - \n",
    "- __date_df_ok (Main Files)__\n",
    "    - Total len: 114,759\n",
    "    - Not in q1_date_df: 82,686 \n",
    "    - Already included: 82,773 SKIPPED as a result\n",
    "- __date_df_ok_final (Main Files)__\n",
    "    - Total len: 114,759\n",
    "    - Not in q1_date_df: 82,686 \n",
    "    - SKIPPED as same as previous\n",
    "    - \n",
    "- __pr_results_df (7e)__\n",
    "    - Total len:102,020\n",
    "    - Not in q1_date_df: 78,645 \n",
    "    - Already included: 4616\n",
    "    - \n",
    "\n",
    "\n",
    "All the articles are concat'd into remainder_df. This df can be used to create FAILED & UNTESTED datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"remainder_df\",\"wb\") as p:\n",
    "    pickle.dump(remainder_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 3.1: remainder_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q1_date_df\", \"rb\") as fp:\n",
    "    q1_date_df = pickle.load(fp)\n",
    "\n",
    "with open(\"remainder_df\", \"rb\") as fp:\n",
    "    remainder_df = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 214301 entries, 10.1007/s10441-011-9133-1 to 10.1214/18-ejp208\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   ISSN                    214301 non-null  object \n",
      " 1   issued                  214301 non-null  object \n",
      " 2   container-title         214301 non-null  object \n",
      " 3   publisher               214294 non-null  object \n",
      " 4   title                   214301 non-null  object \n",
      " 5   author                  192795 non-null  object \n",
      " 6   subject                 213933 non-null  object \n",
      " 7   references-count        118145 non-null  float64\n",
      " 8   is-referenced-by-count  214301 non-null  int64  \n",
      " 9   published               39116 non-null   object \n",
      " 10  published-print         201171 non-null  object \n",
      " 11  published-online        119803 non-null  object \n",
      " 12  direct_url              41047 non-null   object \n",
      " 13  Dates                   41047 non-null   object \n",
      " 14  Keywords                41047 non-null   object \n",
      " 15  Affiliation             4199 non-null    object \n",
      " 16  JRNL_ID                 113903 non-null  object \n",
      " 17  reference-count         96156 non-null   float64\n",
      " 18  reference               60392 non-null   object \n",
      " 19  language                31507 non-null   object \n",
      " 20  Results                 96156 non-null   object \n",
      "dtypes: float64(2), int64(1), object(18)\n",
      "memory usage: 36.0+ MB\n"
     ]
    }
   ],
   "source": [
    "remainder_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnec. colms:\n",
    "remainder_df.drop([\"reference\",\"language\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'list'>    82686\n",
       "<class 'str'>     13470\n",
       "Name: Results, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_df[remainder_df.Results.notna()].Results.map(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    78240\n",
       "3     4446\n",
       "Name: Results, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_df[(remainder_df.Results.notna() & (remainder_df.Results.map(type)==list))][\"Results\"].map(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISSN</th>\n",
       "      <th>issued</th>\n",
       "      <th>container-title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>subject</th>\n",
       "      <th>references-count</th>\n",
       "      <th>is-referenced-by-count</th>\n",
       "      <th>published</th>\n",
       "      <th>published-print</th>\n",
       "      <th>published-online</th>\n",
       "      <th>direct_url</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>JRNL_ID</th>\n",
       "      <th>reference-count</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOI</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ISSN, issued, container-title, publisher, title, author, subject, references-count, is-referenced-by-count, published, published-print, published-online, direct_url, Dates, Keywords, Affiliation, JRNL_ID, reference-count, Results]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_df[(remainder_df.Dates.notna()) & (remainder_df.Results.notna()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISSN</th>\n",
       "      <th>issued</th>\n",
       "      <th>container-title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>subject</th>\n",
       "      <th>references-count</th>\n",
       "      <th>is-referenced-by-count</th>\n",
       "      <th>published</th>\n",
       "      <th>published-print</th>\n",
       "      <th>published-online</th>\n",
       "      <th>direct_url</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>JRNL_ID</th>\n",
       "      <th>reference-count</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOI</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ISSN, issued, container-title, publisher, title, author, subject, references-count, is-referenced-by-count, published, published-print, published-online, direct_url, Dates, Keywords, Affiliation, JRNL_ID, reference-count, Results]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_df[(remainder_df[\"reference-count\"].notna()) & (remainder_df[\"references-count\"].notna()) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__remainder_df__\n",
    "\n",
    "__1- Succ:__\n",
    "- has Dates & type == list\n",
    "- has Results & len == 3\n",
    "\n",
    "__2- Fail:__\n",
    "- has Dates & type==str\n",
    "- has Results & len==2\n",
    "\n",
    "__3- Untested:__\n",
    "- no Results & Dates cols.\n",
    "- DOI not in any other df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remainder_succ:\n",
    "\n",
    "remainder_succ = remainder_df[(remainder_df.Dates.notna() & (remainder_df.Dates.map(type)==list))]\n",
    "\n",
    "remainder_succ = pd.concat([remainder_succ, remainder_df.fillna(\"-\")[(remainder_df.fillna(\"-\").Results.notna() & (remainder_df.Results.fillna(\"-\").map(len)==3))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remainder_fail:\n",
    "\n",
    "remainder_fail = remainder_df[(remainder_df.Dates.notna() & (remainder_df.Dates.map(type)==str))]\n",
    "\n",
    "remainder_fail = pd.concat([remainder_fail, remainder_df.fillna(\"-\")[(remainder_df.fillna(\"-\").Results.notna()) & (remainder_df.Results.fillna(\"-\").map(len)==2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remainder_untested:\n",
    "\n",
    "# Need to drop all DOIs from other datasets\n",
    "\n",
    "date_df_dois = q1_date_df.index.to_list()\n",
    "\n",
    "rem_suc_dois = remainder_succ.index.tolist()\n",
    "\n",
    "rem_fail_dois = remainder_fail.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dois = set(date_df_dois + rem_suc_dois + rem_fail_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder_untested = remainder_df[~remainder_df.index.isin(all_dois)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80243, 19)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_untested.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_not_in_Scihub    9454\n",
       "skipped                  3655\n",
       "cant_read_pdf             242\n",
       "direct_url_error           10\n",
       "pdf_bytes_error             7\n",
       "article_not_in Scihub       2\n",
       "Name: Results, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_untested.Results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76588, 19)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder_untested[remainder_untested.Results.isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that remainder_untested has 89,958 articles remaining. However, due to the filtering conditions some are actually failed articles & will be moved to remainder_fail dataset.\n",
    "\n",
    "The rest can be used in the following steps if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also seen that this step increased the number of untested artcl. count from 41K (pr_results_df) to 80K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder_fail = pd.concat([remainder_fail, remainder_untested[~(remainder_untested.Results.isna()) | (remainder_untested.Results == \"skipped\")]])\n",
    "\n",
    "remainder_untested = remainder_untested[(remainder_untested.Results.isna()) | (remainder_untested.Results == \"skipped\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the remainder datasets are ready for future steps, so each will be saved accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"remainder_df\",\"wb\") as p:\n",
    "    pickle.dump(remainder_df, p)\n",
    "\n",
    "with open(\"remainder_succ\",\"wb\") as p:\n",
    "    pickle.dump(remainder_succ, p)\n",
    "\n",
    "with open(\"remainder_fail\",\"wb\") as p:\n",
    "    pickle.dump(remainder_fail, p)\n",
    "\n",
    "with open(\"remainder_untested\",\"wb\") as p:\n",
    "    pickle.dump(remainder_untested, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d6808e9ba8815475743150367720cad3673ac2b3f4957dc753295ba7ac37a1a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
