{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nb will be used to qucikly access CR+SH and ScopusAS API loops to add new articles as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- CR + SH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from useful_functions import *\n",
    "\n",
    "# from crossref.restful import Works, Etiquette\n",
    "# from scihub_blastoise import SciHub_HydroPump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here:\n",
    "\n",
    "with open(\"q1_date_df\", \"rb\") as fp:\n",
    "    q1_date_df = pickle.load(fp)\n",
    "\n",
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Pivots & Required_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7b_variables/q1_a_ss\", \"rb\") as fp:\n",
    "    q1_a_ss = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pivot = pd.pivot_table(data=q1_date_df, columns=\"JRNL_YEAR\", index=\"Chosen_SA\", values=\"title\", aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pivot = (q1_a_ss - current_pivot).applymap(lambda x: x if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty columns:\n",
    "\n",
    "q1_jrnl_df[\"Required_Count\"] = q1_jrnl_df.Sbj_Area.map(dict)\n",
    "q1_jrnl_df[\"Required_Count\"] = q1_jrnl_df.Required_Count.map(lambda x: dict.fromkeys(x,int(0)))\n",
    "\n",
    "q1_jrnl_df[\"SA_list\"] = q1_jrnl_df.Required_Count.map(lambda x: list(x.keys()))\n",
    "\n",
    "q1_jrnl_df[\"Required_Count_Sum\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributing missing articles to journals:\n",
    "\n",
    "multip = 1.5\n",
    "\n",
    "for s_a, row in missing_pivot.iterrows(): #iterate over rows\n",
    "    # print(s_a)\n",
    "    for year, value in row.items():\n",
    "        # print(year)\n",
    "        if value == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(int(value*multip)):\n",
    "                # select one: \n",
    "                sampl_index = q1_jrnl_df[(q1_jrnl_df.Consider == True) & (q1_jrnl_df.CR_Count > (q1_jrnl_df.Required_Count_Sum + q1_jrnl_df.Current_Count)) & (q1_jrnl_df.SA_list.map(lambda x: s_a in x)) & (q1_jrnl_df.Year == year)].sample(1).index.item()\n",
    "        \n",
    "                q1_jrnl_df.loc[sampl_index,\"Required_Count\"][s_a] += 1\n",
    "                q1_jrnl_df.loc[sampl_index,\"Required_Count_Sum\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Required_df & all_dois & CR + SH Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOI list to be skipped in the loop:\n",
    "\n",
    "all_dois = set(q1_date_df.index.to_list() + remainder_df.index.to_list())\n",
    "\n",
    "with open(\"all_dois\",\"wb\") as p:\n",
    "    pickle.dump(all_dois, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered version of jrnl_df to be used in the loop:\n",
    "\n",
    "required_df = q1_jrnl_df[q1_jrnl_df.Required_Count_Sum > 0]\n",
    "\n",
    "with open(\"required_df\",\"wb\") as p:\n",
    "    pickle.dump(required_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sh_h = SciHub_HydroPump()\n",
    "\n",
    "mail = 'sduzenli@yandex.com.tr'\n",
    "\n",
    "my_etiquette = Etiquette('Analysing Publishing Delay in Academic Journals', 'v2.0', 'https://github.com/Spidey0023/THEsis-Codes', mail)\n",
    "ogi_works = Works(etiquette=my_etiquette)\n",
    "\n",
    "\n",
    "partial_df = required_df[[\"ISSN\",\"Year\", \"Required_Count_Sum\",\"CR_Count\"]]\n",
    "\n",
    "partial_df.loc[:,\"ISSN\"] = partial_df.ISSN.map(list)\n",
    "pr_dict = partial_df.to_dict(\"index\")\n",
    "\n",
    "new_run_cr_results = list()\n",
    "new_run_sh_results = list() \n",
    "\n",
    "\n",
    "useful_cols = ['DOI', 'references-count', 'publisher', 'published-print', 'is-referenced-by-count', 'title', 'author', 'published-online', 'container-title', 'issued', 'ISSN', 'subject']\n",
    "\n",
    "for key in pr_dict:\n",
    "    issn = pr_dict[key][\"ISSN\"]\n",
    "    year = pr_dict[key][\"Year\"]\n",
    "\n",
    "    rc = pr_dict[key][\"Required_Count_Sum\"]\n",
    "    crc = pr_dict[key][\"CR_Count\"]\n",
    "    min_rc = min(rc+3, crc)\n",
    "\n",
    "    loop_meta = [w for w in ogi_works.filter(issn=issn, from_pub_date=year, until_pub_date=year, type=\"journal-article\").sample(int(min_rc)).select(useful_cols)]\n",
    "    loop_meta = [dict(item, JRNL_ID=key) for item in loop_meta if item[\"DOI\"] not in all_dois]\n",
    "    new_run_cr_results.extend(loop_meta)\n",
    "\n",
    "    for artcl in loop_meta:\n",
    "        doi = artcl[\"DOI\"]\n",
    "        pdf_url, date, kw = sh_h.get_dates_kws(doi)\n",
    "        new_run_sh_results.append(dict(DOI = doi, direct_url = pdf_url, Dates = date, Keywords = kw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_req_cr_results\",\"wb\") as fp:\n",
    "   pickle.dump(new_run_cr_results, fp)\n",
    "\n",
    "with open(\"new_req_sh_results\",\"wb\") as fp:\n",
    "   pickle.dump(new_run_sh_results, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Run Results' Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_req_cr_results_final\", \"rb\") as fp:\n",
    "    new_run_cr_results_final = pickle.load(fp)\n",
    "\n",
    "with open(\"new_req_sh_results_final\", \"rb\") as fp:\n",
    "    new_run_sh_results_final = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df (CR + SH):\n",
    "new_run_df = pd.DataFrame(new_run_cr_results_final).merge(pd.DataFrame(new_run_sh_results_final), how=\"inner\", on=\"DOI\")\n",
    "\n",
    "# to be added to date_df:\n",
    "new_run_succ = new_run_df[new_run_df.Dates.map(lambda x: type(x) == list)]\n",
    "new_run_succ = new_run_succ[new_run_succ.Dates.map(lambda x: type(x[0]) == str)]\n",
    "\n",
    "# to be added to remainder_df:\n",
    "new_run_rem = new_run_df[~new_run_df.DOI.isin(new_run_succ.index.to_list())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_run_cr_results_final\n",
    "del new_run_sh_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format engineering on the new_run_succ dataset:\n",
    "\n",
    "new_run_succ.set_index(\"DOI\", drop=True, inplace=True)\n",
    "new_run_succ.loc[:,\"published\"] = new_run_succ.apply(earlier_date_part, axis=1)\n",
    "new_run_succ = new_run_succ[[\"JRNL_ID\", \"ISSN\", \"issued\", \"container-title\", \"publisher\", \"title\", \"author\", \"subject\", \"references-count\", \"is-referenced-by-count\", \"published\", \"published-print\", \"published-online\", \"direct_url\", \"Dates\", \"Keywords\"]]\n",
    "new_run_succ.loc[:,\"JRNL_YEAR\"] = new_run_succ.JRNL_ID.map(q1_jrnl_df.Year.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning subj. areas to successful articles:\n",
    "\n",
    "for id, row in new_run_succ.iterrows():\n",
    "    jrnl_id = row[\"JRNL_ID\"]\n",
    "    rc_dict = q1_jrnl_df.loc[jrnl_id,\"Required_Count\"]\n",
    "    \n",
    "\n",
    "    if sum(list(rc_dict.values())) == 0:\n",
    "        new_run_succ.loc[id,\"Chosen_SA\"] = choice(list(rc_dict.keys()))\n",
    "\n",
    "    else:\n",
    "        chosen_sa = choice([k for k,v in rc_dict.items() if v>0])\n",
    "        q1_jrnl_df.loc[jrnl_id,\"Required_Count\"][chosen_sa] -= 1\n",
    "        new_run_succ.loc[id,\"Chosen_SA\"] = chosen_sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small modifications on jrnl_df & saving:\n",
    "\n",
    "q1_jrnl_df[\"New_Additions\"] = new_run_succ.groupby(\"JRNL_ID\")[\"title\"].count()\n",
    "q1_jrnl_df.New_Additions.fillna(0, inplace=True)\n",
    "q1_jrnl_df[\"New_Additions\"] = q1_jrnl_df.New_Additions.astype(int)\n",
    "\n",
    "with open(\"q1_jrnl_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_jrnl_df, p)\n",
    "\n",
    "with open(\"q1_jrnl_df_copy\",\"wb\") as p:\n",
    "        pickle.dump(q1_jrnl_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new articles to date_df & saving:\n",
    "\n",
    "q1_date_df = pd.concat([q1_date_df,new_run_succ],verify_integrity=True)\n",
    "\n",
    "with open(\"q1_date_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_date_df, p)\n",
    "\n",
    "with open(\"q1_date_df_copy\",\"wb\") as p:\n",
    "        pickle.dump(q1_date_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new articles to remainder_df & saving:\n",
    "\n",
    "with open(\"q1_remainder_df\", \"rb\") as fp:\n",
    "    q1_remainder_df = pickle.load(fp)\n",
    "\n",
    "q1_remainder_df = pd.concat([q1_remainder_df, new_run_rem.set_index(\"DOI\")], verify_integrity=True)\n",
    "\n",
    "with open(\"q1_remainder_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_remainder_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving datasets to the run folder:\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/k5k_new_df\",\"wb\") as p:\n",
    "#         pickle.dump(k5k_new_df, p)\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/new_run_succ\",\"wb\") as p:\n",
    "#         pickle.dump(new_run_succ, p)\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/k5k_new_rem_df\",\"wb\") as p:\n",
    "#         pickle.dump(k5k_new_rem_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unnec. variables:\n",
    "\n",
    "del q1_remainder_df\n",
    "del new_run_df\n",
    "del new_run_succ\n",
    "del new_run_rem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Scopus Author Search API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - author_df & first_auhtor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pickle\n",
    "\n",
    "from useful_functions import *\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elssearch import ElsSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7e_variables/new_run_succ\", \"rb\") as fp:\n",
    "    new_run_succ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new_author_df from new_suc_df\n",
    "\n",
    "new_author_df = new_run_succ.dropna(subset= \"author\").reset_index().loc[:,[\"DOI\", \"JRNL_ID\", \"author\"]].explode(\"author\").reset_index(drop=True)\n",
    "new_aut_df = pd.DataFrame(list(new_author_df['author']))\n",
    "\n",
    "# Combining the two datasets:\n",
    "\n",
    "new_author_df = pd.concat([new_author_df, new_aut_df], axis=1)\n",
    "new_author_df.drop([\"suffix\", \"name\", \"author\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop empty given & family:\n",
    "\n",
    "new_author_df.dropna(subset=[\"family\", \"given\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip():\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "# Remove multiple spaces:\n",
    "new_author_df.given = new_author_df.given.map(lambda x: re.sub(' +', ' ', x))\n",
    "new_author_df.family = new_author_df.family.map(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "\n",
    "# Case 1: A. C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x)))].given.map(lambda x: x +\".\")\n",
    "# Case 2: A.C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x)))].given.map(lambda x: x.replace(\".\",\". \") +\".\")\n",
    "# Case 3: AC\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x)))].given.map(lambda x: x[0] + \". \" + x[1] + \".\")\n",
    "# Case 4: A C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x)))].given.map(lambda x: x.replace(\" \", \". \") + \".\")\n",
    "# Case 5: A.C.\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x))), \"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x)))].given.map(lambda x: x.replace(\".\", \". \").strip())\n",
    "\n",
    "# Single letter names:\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x)))].given.map(lambda x: x + \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Simplifier:\n",
    "\n",
    "new_author_df.loc[:,\"new_given\"] = new_author_df.given.map(name_simplifier)\n",
    "new_author_df.loc[:,\"new_family\"] = new_author_df.family.map(name_simplifier)\n",
    "\n",
    "# After process stripping:\n",
    "\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "new_author_df.new_given = new_author_df.new_given.str.strip()\n",
    "new_author_df.new_family = new_author_df.new_family.str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Subj. Area column to the new_author_df:\n",
    "\n",
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)\n",
    "\n",
    "new_author_df = pd.merge(left=new_author_df, right=q1_jrnl_df[\"SA_list\"], how=\"left\", left_on=\"JRNL_ID\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new_first_author_df\n",
    "\n",
    "\n",
    "new_first_author_df = new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"SA_list\"].apply(list)\n",
    "new_first_author_df = new_first_author_df.to_frame().merge(new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"DOI\"].apply(list), how= \"inner\", right_index=True, left_index=True)\n",
    "new_first_author_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any problamatic names:\n",
    "\n",
    "new_first_author_df[new_first_author_df.new_given.map(lambda x: bool(re.match(\"\\W\",x)))]\n",
    "new_first_author_df[new_first_author_df.new_family.map(lambda x: bool(re.match(\"\\W\",x)))]\n",
    "\n",
    "# if found:\n",
    "new_first_author_df.drop(24266, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Flattening SA_list col:\n",
    "\n",
    "new_first_author_df.loc[:,\"SA_list\"] = new_first_author_df.SA_list.map(flatten)\n",
    "\n",
    "# 2- Creating an Author ID col:\n",
    "\n",
    "new_first_author_df.reset_index(drop=True, inplace=True)\n",
    "new_first_author_df.reset_index(drop=False,inplace=True)\n",
    "new_first_author_df.loc[:,\"index\"] = new_first_author_df.index.map(lambda x: \"aut_\" + str(x))\n",
    "new_first_author_df.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_first_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_first_author_df, p)\n",
    "\n",
    "with open(\"new_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_author_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Scopus loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_input_dict = new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Loop - Improved:\n",
    "\n",
    "new_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"8707db153e4b9672fa6df25b03a5f747\",\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "used_api_keys_list = list()\n",
    "\n",
    "current_key = avail_api_keys_list.pop(0)\n",
    "client = ElsClient(current_key)\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in new_author_input_dict:\n",
    "    given_in = new_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = new_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = new_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "        cnt_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        err_no = re.search(\"\\d+\",str(e))[0]\n",
    "\n",
    "        print(f\"Encountered Error: {err_no}!\")\n",
    "        print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "        with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(new_author_out_list, p)\n",
    "        print(\"Loop save complete!\")\n",
    "\n",
    "        if \"400\" in str(e):\n",
    "            # query error, need to skip\n",
    "            print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "            cnt_check = False\n",
    "\n",
    "        elif \"401\" in str(e):\n",
    "            # VPN error, sleep try again\n",
    "            sleep(60)\n",
    "            print(\"Slept for 60 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                print(\"Got an error again & breaking the loop ! :(\")\n",
    "                break\n",
    "            \n",
    "        elif \"429\" in str(e):\n",
    "            # change API key \n",
    "            while len(avail_api_keys_list) >0:\n",
    "                used_api_keys_list.append(current_key)\n",
    "                current_key = avail_api_keys_list.pop(0)\n",
    "                client = ElsClient(current_key)\n",
    "                print(f\"Changed API key, now using key: {current_key}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"New key is OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"429\":\n",
    "                        print(\"New key is also finished!\")\n",
    "                    else:\n",
    "                        print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "                        cnt_check = False\n",
    "                        break\n",
    "\n",
    "            if len(avail_api_keys_list) == 0:\n",
    "                print(\"No available API keys left & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"500\" in str(e):\n",
    "            sleep(30)\n",
    "            print(\"Slept for 30 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                cnt_check = False\n",
    "            pass\n",
    "        \n",
    "        elif \"10060\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown error & breaking the loop! :(\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if cnt_check:\n",
    "        country_out = list()\n",
    "        given_out = list()\n",
    "        family_out = list()\n",
    "        doc_count_out = list()\n",
    "\n",
    "        for auth in auth_srch.results:\n",
    "            # Country:\n",
    "            try:\n",
    "                country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Given:\n",
    "            try:\n",
    "                given_out.append(auth['preferred-name']['given-name'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Family:\n",
    "            try:\n",
    "                family_out.append(auth['preferred-name']['surname'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Doc Count:\n",
    "            try:\n",
    "                doc_count_out.append(auth['document-count'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "        new_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check progress in case of an error:\n",
    "\n",
    "pd.DataFrame(new_author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_author_out_list\",\"wb\") as p:\n",
    "    pickle.dump(new_author_out_list, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Combining Scopus results datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading if necessary:\n",
    "\n",
    "with open(\"location\", \"rb\") as fp:\n",
    "    new_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"location\", \"rb\") as fp:\n",
    "    new_first_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"location, \"rb\") as fp:\n",
    "    new_author_out_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_author_df = new_first_author_df.merge(pd.DataFrame(new_author_out_list).set_index(\"author_id\"), how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "# Then combine with the main first_author_df as necessary:\n",
    "all_first_author_df = pd.concat([all_first_author_df,new_first_author_df], ignore_index=True, verify_integrity=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d6808e9ba8815475743150367720cad3673ac2b3f4957dc753295ba7ac37a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
