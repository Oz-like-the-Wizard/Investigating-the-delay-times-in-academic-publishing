{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nb will be used to qucikly access CR+SH and ScopusAS API loops to add new articles as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import regex as re\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from useful_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- CR + SH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from useful_functions import *\n",
    "\n",
    "# from crossref.restful import Works, Etiquette\n",
    "# from scihub_blastoise import SciHub_HydroPump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here:\n",
    "\n",
    "with open(\"q1_date_df\", \"rb\") as fp:\n",
    "    q1_date_df = pickle.load(fp)\n",
    "\n",
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Pivots & Required_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7b_variables/q1_a_ss\", \"rb\") as fp:\n",
    "    q1_a_ss = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pivot = pd.pivot_table(data=q1_date_df, columns=\"JRNL_YEAR\", index=\"Chosen_SA\", values=\"title\", aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pivot = (q1_a_ss - current_pivot).applymap(lambda x: x if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty columns:\n",
    "\n",
    "q1_jrnl_df[\"Required_Count\"] = q1_jrnl_df.Sbj_Area.map(dict)\n",
    "q1_jrnl_df[\"Required_Count\"] = q1_jrnl_df.Required_Count.map(lambda x: dict.fromkeys(x,int(0)))\n",
    "\n",
    "q1_jrnl_df[\"SA_list\"] = q1_jrnl_df.Required_Count.map(lambda x: list(x.keys()))\n",
    "\n",
    "q1_jrnl_df[\"Required_Count_Sum\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributing missing articles to journals:\n",
    "\n",
    "multip = 1.5\n",
    "\n",
    "for s_a, row in missing_pivot.iterrows(): #iterate over rows\n",
    "    # print(s_a)\n",
    "    for year, value in row.items():\n",
    "        # print(year)\n",
    "        if value == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(int(value*multip)):\n",
    "                # select one: \n",
    "                sampl_index = q1_jrnl_df[(q1_jrnl_df.Consider == True) & (q1_jrnl_df.CR_Count > (q1_jrnl_df.Required_Count_Sum + q1_jrnl_df.Current_Count)) & (q1_jrnl_df.SA_list.map(lambda x: s_a in x)) & (q1_jrnl_df.Year == year)].sample(1).index.item()\n",
    "        \n",
    "                q1_jrnl_df.loc[sampl_index,\"Required_Count\"][s_a] += 1\n",
    "                q1_jrnl_df.loc[sampl_index,\"Required_Count_Sum\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Required_df & all_dois & CR + SH Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOI list to be skipped in the loop:\n",
    "\n",
    "all_dois = set(q1_date_df.index.to_list() + remainder_df.index.to_list())\n",
    "\n",
    "with open(\"all_dois\",\"wb\") as p:\n",
    "    pickle.dump(all_dois, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered version of jrnl_df to be used in the loop:\n",
    "\n",
    "required_df = q1_jrnl_df[q1_jrnl_df.Required_Count_Sum > 0]\n",
    "\n",
    "with open(\"required_df\",\"wb\") as p:\n",
    "    pickle.dump(required_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sh_h = SciHub_HydroPump()\n",
    "\n",
    "mail = 'sduzenli@yandex.com.tr'\n",
    "\n",
    "my_etiquette = Etiquette('Analysing Publishing Delay in Academic Journals', 'v2.0', 'https://github.com/Spidey0023/THEsis-Codes', mail)\n",
    "ogi_works = Works(etiquette=my_etiquette)\n",
    "\n",
    "\n",
    "partial_df = required_df[[\"ISSN\",\"Year\", \"Required_Count_Sum\",\"CR_Count\"]]\n",
    "\n",
    "partial_df.loc[:,\"ISSN\"] = partial_df.ISSN.map(list)\n",
    "pr_dict = partial_df.to_dict(\"index\")\n",
    "\n",
    "new_run_cr_results = list()\n",
    "new_run_sh_results = list() \n",
    "\n",
    "\n",
    "useful_cols = ['DOI', 'references-count', 'publisher', 'published-print', 'is-referenced-by-count', 'title', 'author', 'published-online', 'container-title', 'issued', 'ISSN', 'subject']\n",
    "\n",
    "for key in pr_dict:\n",
    "    issn = pr_dict[key][\"ISSN\"]\n",
    "    year = pr_dict[key][\"Year\"]\n",
    "\n",
    "    rc = pr_dict[key][\"Required_Count_Sum\"]\n",
    "    crc = pr_dict[key][\"CR_Count\"]\n",
    "    min_rc = min(rc+3, crc)\n",
    "\n",
    "    loop_meta = [w for w in ogi_works.filter(issn=issn, from_pub_date=year, until_pub_date=year, type=\"journal-article\").sample(int(min_rc)).select(useful_cols)]\n",
    "    loop_meta = [dict(item, JRNL_ID=key) for item in loop_meta if item[\"DOI\"] not in all_dois]\n",
    "    new_run_cr_results.extend(loop_meta)\n",
    "\n",
    "    for artcl in loop_meta:\n",
    "        doi = artcl[\"DOI\"]\n",
    "        pdf_url, date, kw = sh_h.get_dates_kws(doi)\n",
    "        new_run_sh_results.append(dict(DOI = doi, direct_url = pdf_url, Dates = date, Keywords = kw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_req_cr_results\",\"wb\") as fp:\n",
    "   pickle.dump(new_run_cr_results, fp)\n",
    "\n",
    "with open(\"new_req_sh_results\",\"wb\") as fp:\n",
    "   pickle.dump(new_run_sh_results, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Run Results' Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_req_cr_results_final\", \"rb\") as fp:\n",
    "    new_run_cr_results_final = pickle.load(fp)\n",
    "\n",
    "with open(\"new_req_sh_results_final\", \"rb\") as fp:\n",
    "    new_run_sh_results_final = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df (CR + SH):\n",
    "new_run_df = pd.DataFrame(new_run_cr_results_final).merge(pd.DataFrame(new_run_sh_results_final), how=\"inner\", on=\"DOI\")\n",
    "\n",
    "# to be added to date_df:\n",
    "new_run_succ = new_run_df[new_run_df.Dates.map(lambda x: type(x) == list)]\n",
    "new_run_succ = new_run_succ[new_run_succ.Dates.map(lambda x: type(x[0]) == str)]\n",
    "\n",
    "# to be added to remainder_df:\n",
    "new_run_rem = new_run_df[~new_run_df.DOI.isin(new_run_succ.DOI.to_list())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_run_cr_results_final\n",
    "del new_run_sh_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format engineering on the new_run_succ dataset:\n",
    "\n",
    "new_run_succ.set_index(\"DOI\", drop=True, inplace=True)\n",
    "new_run_succ.loc[:,\"published\"] = new_run_succ.apply(earlier_date_part, axis=1)\n",
    "new_run_succ = new_run_succ[[\"JRNL_ID\", \"ISSN\", \"issued\", \"container-title\", \"publisher\", \"title\", \"author\", \"subject\", \"references-count\", \"is-referenced-by-count\", \"published\", \"published-print\", \"published-online\", \"direct_url\", \"Dates\", \"Keywords\"]]\n",
    "new_run_succ.loc[:,\"JRNL_YEAR\"] = new_run_succ.JRNL_ID.map(q1_jrnl_df.Year.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning subj. areas to successful articles:\n",
    "\n",
    "for id, row in new_run_succ.iterrows():\n",
    "    jrnl_id = row[\"JRNL_ID\"]\n",
    "    rc_dict = q1_jrnl_df.loc[jrnl_id,\"Required_Count\"]\n",
    "    \n",
    "\n",
    "    if sum(list(rc_dict.values())) == 0:\n",
    "        new_run_succ.loc[id,\"Chosen_SA\"] = choice(list(rc_dict.keys()))\n",
    "\n",
    "    else:\n",
    "        chosen_sa = choice([k for k,v in rc_dict.items() if v>0])\n",
    "        q1_jrnl_df.loc[jrnl_id,\"Required_Count\"][chosen_sa] -= 1\n",
    "        new_run_succ.loc[id,\"Chosen_SA\"] = chosen_sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small modifications on jrnl_df & saving:\n",
    "\n",
    "q1_jrnl_df[\"New_Additions\"] = new_run_succ.groupby(\"JRNL_ID\")[\"title\"].count()\n",
    "q1_jrnl_df.New_Additions.fillna(0, inplace=True)\n",
    "q1_jrnl_df[\"New_Additions\"] = q1_jrnl_df.New_Additions.astype(int)\n",
    "\n",
    "with open(\"q1_jrnl_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_jrnl_df, p)\n",
    "\n",
    "with open(\"q1_jrnl_df_copy\",\"wb\") as p:\n",
    "        pickle.dump(q1_jrnl_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new articles to date_df & saving:\n",
    "\n",
    "q1_date_df = pd.concat([q1_date_df,new_run_succ],verify_integrity=True)\n",
    "\n",
    "with open(\"q1_date_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_date_df, p)\n",
    "\n",
    "with open(\"q1_date_df_copy\",\"wb\") as p:\n",
    "        pickle.dump(q1_date_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new articles to remainder_df & saving:\n",
    "\n",
    "with open(\"q1_remainder_df\", \"rb\") as fp:\n",
    "    q1_remainder_df = pickle.load(fp)\n",
    "\n",
    "q1_remainder_df = pd.concat([q1_remainder_df, new_run_rem.set_index(\"DOI\")], verify_integrity=True)\n",
    "\n",
    "with open(\"q1_remainder_df\",\"wb\") as p:\n",
    "        pickle.dump(q1_remainder_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving datasets to the run folder:\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/k5k_new_df\",\"wb\") as p:\n",
    "#         pickle.dump(k5k_new_df, p)\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/new_run_succ\",\"wb\") as p:\n",
    "#         pickle.dump(new_run_succ, p)\n",
    "\n",
    "# with open(\"7e_variables/last_5k_run_Sum/k5k_new_rem_df\",\"wb\") as p:\n",
    "#         pickle.dump(k5k_new_rem_df, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unnec. variables:\n",
    "\n",
    "del q1_remainder_df\n",
    "del new_run_df\n",
    "del new_run_succ\n",
    "del new_run_rem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Scopus Author Search API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - author_df & first_auhtor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pickle\n",
    "\n",
    "from useful_functions import *\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elssearch import ElsSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7e_variables/new_run_succ\", \"rb\") as fp:\n",
    "    new_run_succ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new_author_df from new_suc_df\n",
    "\n",
    "new_author_df = new_run_succ.dropna(subset= \"author\").reset_index().loc[:,[\"DOI\", \"JRNL_ID\", \"author\"]].explode(\"author\").reset_index(drop=True)\n",
    "new_aut_df = pd.DataFrame(list(new_author_df['author']))\n",
    "\n",
    "# Combining the two datasets:\n",
    "\n",
    "new_author_df = pd.concat([new_author_df, new_aut_df], axis=1)\n",
    "new_author_df.drop([\"suffix\", \"name\", \"author\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop empty given & family:\n",
    "\n",
    "new_author_df.dropna(subset=[\"family\", \"given\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip():\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "# Remove multiple spaces:\n",
    "new_author_df.given = new_author_df.given.map(lambda x: re.sub(' +', ' ', x))\n",
    "new_author_df.family = new_author_df.family.map(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "\n",
    "# Case 1: A. C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.\\s[A-Z]$\", x)))].given.map(lambda x: x +\".\")\n",
    "# Case 2: A.C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]$\", x)))].given.map(lambda x: x.replace(\".\",\". \") +\".\")\n",
    "# Case 3: AC\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]{2}$\", x)))].given.map(lambda x: x[0] + \". \" + x[1] + \".\")\n",
    "# Case 4: A C\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\s[A-Z]$\", x)))].given.map(lambda x: x.replace(\" \", \". \") + \".\")\n",
    "# Case 5: A.C.\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x))), \"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]\\.[A-Z]\\.$\", x)))].given.map(lambda x: x.replace(\".\", \". \").strip())\n",
    "\n",
    "# Single letter names:\n",
    "new_author_df.loc[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x))),\"new_given\"] = new_author_df[new_author_df.given.map(lambda x: bool(re.match(\"^[A-Z]$\", x)))].given.map(lambda x: x + \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Simplifier:\n",
    "\n",
    "new_author_df.loc[:,\"new_given\"] = new_author_df.given.map(name_simplifier)\n",
    "new_author_df.loc[:,\"new_family\"] = new_author_df.family.map(name_simplifier)\n",
    "\n",
    "# After process stripping:\n",
    "\n",
    "new_author_df.given = new_author_df.given.str.strip()\n",
    "new_author_df.family = new_author_df.family.str.strip()\n",
    "\n",
    "new_author_df.new_given = new_author_df.new_given.str.strip()\n",
    "new_author_df.new_family = new_author_df.new_family.str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Subj. Area column to the new_author_df:\n",
    "\n",
    "with open(\"q1_jrnl_df\", \"rb\") as fp:\n",
    "    q1_jrnl_df = pickle.load(fp)\n",
    "\n",
    "new_author_df = pd.merge(left=new_author_df, right=q1_jrnl_df[\"SA_list\"], how=\"left\", left_on=\"JRNL_ID\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new_first_author_df\n",
    "\n",
    "\n",
    "new_first_author_df = new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"SA_list\"].apply(list)\n",
    "new_first_author_df = new_first_author_df.to_frame().merge(new_author_df[new_author_df.sequence == \"first\"].groupby([\"new_given\", \"new_family\"])[\"DOI\"].apply(list), how= \"inner\", right_index=True, left_index=True)\n",
    "new_first_author_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any problamatic names:\n",
    "\n",
    "new_first_author_df[new_first_author_df.new_given.map(lambda x: bool(re.match(\"\\W\",x)))]\n",
    "new_first_author_df[new_first_author_df.new_family.map(lambda x: bool(re.match(\"\\W\",x)))]\n",
    "\n",
    "# if found:\n",
    "new_first_author_df.drop(24266, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Flattening SA_list col:\n",
    "\n",
    "new_first_author_df.loc[:,\"SA_list\"] = new_first_author_df.SA_list.map(flatten)\n",
    "\n",
    "# 2- Creating an Author ID col:\n",
    "\n",
    "new_first_author_df.reset_index(drop=True, inplace=True)\n",
    "new_first_author_df.reset_index(drop=False,inplace=True)\n",
    "new_first_author_df.loc[:,\"index\"] = new_first_author_df.index.map(lambda x: \"aut_\" + str(x))\n",
    "new_first_author_df.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_first_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_first_author_df, p)\n",
    "\n",
    "with open(\"new_author_df\",\"wb\") as p:\n",
    "    pickle.dump(new_author_df, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Scopus loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_input_dict = new_first_author_df[[\"new_given\",\"new_family\",\"SA_list\"]].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Loop - Improved:\n",
    "\n",
    "new_author_out_list= list()\n",
    "\n",
    "avail_api_keys_list = [\"8707db153e4b9672fa6df25b03a5f747\",\"2dab4694b4347fa574d159bb97484fc4\",\"6af3d2c09eb08ec6e12f0cead9a1f5bb\",\"3872e798bf48fa28af583b9ebef5deb6\",\"1473c31dbcdb425f9cdaf75c673279d3\"]\n",
    "\n",
    "used_api_keys_list = list()\n",
    "\n",
    "current_key = avail_api_keys_list.pop(0)\n",
    "client = ElsClient(current_key)\n",
    "\n",
    "print(f'Start Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "for auth_id in new_author_input_dict:\n",
    "    given_in = new_author_input_dict[auth_id][\"new_given\"]\n",
    "    family_in = new_author_input_dict[auth_id][\"new_family\"]\n",
    "    sa_in = new_author_input_dict[auth_id][\"SA_list\"]\n",
    "\n",
    "    try:\n",
    "        auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "        auth_srch.execute(client)\n",
    "        cnt_check = True\n",
    "\n",
    "    except Exception as e:\n",
    "        err_no = re.search(\"\\d+\",str(e))[0]\n",
    "\n",
    "        print(f\"Encountered Error: {err_no}!\")\n",
    "        print(f'Time = {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "        with open(\"new_author_out_list_loop_save\",\"wb\") as p:\n",
    "                pickle.dump(new_author_out_list, p)\n",
    "        print(\"Loop save complete!\")\n",
    "\n",
    "        if \"400\" in str(e):\n",
    "            # query error, need to skip\n",
    "            print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "            cnt_check = False\n",
    "\n",
    "        elif \"401\" in str(e):\n",
    "            # VPN error, sleep try again\n",
    "            sleep(60)\n",
    "            print(\"Slept for 60 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                print(\"Got an error again & breaking the loop ! :(\")\n",
    "                break\n",
    "            \n",
    "        elif \"429\" in str(e):\n",
    "            # change API key \n",
    "            while len(avail_api_keys_list) >0:\n",
    "                used_api_keys_list.append(current_key)\n",
    "                current_key = avail_api_keys_list.pop(0)\n",
    "                client = ElsClient(current_key)\n",
    "                print(f\"Changed API key, now using key: {current_key}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"New key is OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"429\":\n",
    "                        print(\"New key is also finished!\")\n",
    "                    else:\n",
    "                        print(f\"Skipping author: {given_in} {family_in} - {auth_id}\")\n",
    "                        cnt_check = False\n",
    "                        break\n",
    "\n",
    "            if len(avail_api_keys_list) == 0:\n",
    "                print(\"No available API keys left & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"500\" in str(e):\n",
    "            sleep(30)\n",
    "            print(\"Slept for 30 & trying again!\")\n",
    "            try:\n",
    "                auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                auth_srch.execute(client)\n",
    "                cnt_check = True\n",
    "            except:\n",
    "                cnt_check = False\n",
    "            pass\n",
    "        \n",
    "        elif \"10060\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "\n",
    "        elif \"10054\" in str(e):\n",
    "            cnt_check = False\n",
    "            for i in range(10):\n",
    "                sleep(30)\n",
    "                print(\"Slept for 30 & trying again!\")\n",
    "                print(f\"Trial number: {i+1}\")\n",
    "                try:\n",
    "                    auth_srch = ElsSearch(f'AUTHLAST({family_in}) AND AUTHFIRST({given_in}) AND {sbj_area_query_creator(sa_in)}','author')\n",
    "                    auth_srch.execute(client)\n",
    "                    cnt_check = True\n",
    "                    print(\"It is now OK!\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    err_no = re.search(\"\\d+\",str(e))[0]\n",
    "                    if err_no == \"10060\":\n",
    "                        print(f\"It is still {err_no}!\")\n",
    "                    else:\n",
    "                        print(f\"Another error: {err_no}\")\n",
    "            \n",
    "            if not cnt_check:\n",
    "                print(f\"Couldn't fix {err_no} & breaking the loop! :(\")\n",
    "                break\n",
    "       \n",
    "        else:\n",
    "            print(\"Unknown error & breaking the loop! :(\")\n",
    "            break\n",
    "\n",
    "\n",
    "    if cnt_check:\n",
    "        country_out = list()\n",
    "        given_out = list()\n",
    "        family_out = list()\n",
    "        doc_count_out = list()\n",
    "\n",
    "        for auth in auth_srch.results:\n",
    "            # Country:\n",
    "            try:\n",
    "                country_out.append(auth['affiliation-current']['affiliation-country'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Given:\n",
    "            try:\n",
    "                given_out.append(auth['preferred-name']['given-name'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Family:\n",
    "            try:\n",
    "                family_out.append(auth['preferred-name']['surname'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Doc Count:\n",
    "            try:\n",
    "                doc_count_out.append(auth['document-count'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        author_out_dict = dict(author_id=auth_id, country=country_out, given=given_out, family=family_out, doc_count=doc_count_out)\n",
    "        new_author_out_list.append(author_out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check progress in case of an error:\n",
    "\n",
    "pd.DataFrame(new_author_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_author_out_list\",\"wb\") as p:\n",
    "    pickle.dump(new_author_out_list, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Combining Scopus results datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading if necessary:\n",
    "\n",
    "with open(\"location\", \"rb\") as fp:\n",
    "    new_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"location\", \"rb\") as fp:\n",
    "    new_first_author_df = pickle.load(fp)\n",
    "\n",
    "with open(\"location\", \"rb\") as fp:\n",
    "    new_author_out_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_author_df = new_first_author_df.merge(pd.DataFrame(new_author_out_list).set_index(\"author_id\"), how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then combine with the main first_author_df as necessary:\n",
    "all_first_author_df = pd.concat([all_first_author_df,new_first_author_df], ignore_index=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Analysis & Graphing Updates (from 8b):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Published Year Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating published year col:\n",
    "q1_date_df[\"publ_year\"] = q1_date_df.published.map(lambda x: x[\"date-parts\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[q1_date_df.publ_year != q1_date_df.JRNL_YEAR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnec. col:\n",
    "q1_date_df.drop(\"publ_year\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Date & datetime columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Date cols:\n",
    "\n",
    "q1_date_df[\"Submitted_Date\"] = q1_date_df.Dates.map(lambda x: x[0])\n",
    "q1_date_df[\"Accepted_Date\"] = q1_date_df.Dates.map(lambda x: x[1])\n",
    "\n",
    "q1_date_df[\"Published_Date\"] = q1_date_df.published.map(lambda x: x[\"date-parts\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datetime cols:\n",
    "q1_date_df[\"Submitted_datetime\"] = q1_date_df.Submitted_Date.map(main_subd_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[q1_date_df.Submitted_datetime.isna()].Submitted_Date.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[\"Published_datetime\"] = q1_date_df.Published_Date.map(publ_date_filler)\n",
    "q1_date_df[\"Published_datetime\"] = q1_date_df.Published_datetime.map(lambda x: \"-\".join([str(it) for it in x])).map(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q1_date_df[q1_date_df.Submitted_datetime > q1_date_df.Published_datetime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 doi_aut_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q1_first_author_df.dropna(subset=\"country\", inplace=True)\n",
    "\n",
    "# Dropping authors with only None country:\n",
    "q1_first_author_df = q1_first_author_df[~((q1_first_author_df.country.map(lambda x: None in x)) & (q1_first_author_df.country.map(len)== 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1_first_author_df.loc[:,\"index\"] = q1_first_author_df.index.map(lambda x: \"aut_\" + str(x))\n",
    "# q1_first_author_df.set_index(\"index\", inplace=True)\n",
    "\n",
    "#  Creating a auth & doi mapping dict w/ only valid authors considered:\n",
    "\n",
    "q1_first_author_df.loc[:,\"consider\"] = q1_first_author_df.country.map(lambda x: True if len(x)>0 else False)\n",
    "q1_first_author_df.consider.fillna(False, inplace=True)\n",
    "\n",
    "doi_aut_dict = dict()\n",
    "\n",
    "for aut_id, row in q1_first_author_df[q1_first_author_df.consider == True].iterrows():\n",
    "    doi_list = row[\"DOI\"]\n",
    "    for doi in doi_list:\n",
    "        if doi in doi_aut_dict.keys():\n",
    "            doi_aut_dict[doi].append(aut_id)\n",
    "        else:\n",
    "            doi_aut_dict[doi] = [aut_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[\"AUTH_ID\"] = q1_date_df.index.map(doi_aut_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing no AUTH'd articles:\n",
    "\n",
    "q1_date_df = q1_date_df[q1_date_df.AUTH_ID.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 date_df masking & updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year_mask = q1_date_df_wip.publ_year == q1_date_df_wip.JRNL_YEAR\n",
    "prob_subd_mask = q1_date_df.Submitted_datetime.notna()\n",
    "subm_publ_mask = q1_date_df.Submitted_datetime < q1_date_df.Published_datetime\n",
    "auth_mask = q1_date_df.AUTH_ID.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remainder_df updates:\n",
    "\n",
    "q1_remainder_df = pickle_loader(\"q1_remainder_df\")\n",
    "q1_remainder_df = pd.concat([q1_remainder_df, q1_date_df[~(prob_subd_mask & subm_publ_mask & auth_mask)]])\n",
    "pickle_dumper(q1_remainder_df, \"q1_remainder_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df = q1_date_df[prob_subd_mask & subm_publ_mask & auth_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df = q1_date_df[['JRNL_ID', 'issued', 'container-title', 'publisher', 'title',\n",
    "       'author', 'subject', 'references-count', 'is-referenced-by-count',\n",
    "       'published', 'published-print', 'published-online', 'direct_url',\n",
    "       'Keywords', 'Chosen_SA', 'AUTH_ID', 'Submitted_Date',\n",
    "       'Accepted_Date', 'Published_Date', 'Submitted_datetime',\n",
    "       'Published_datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dumper(q1_date_df, \"q1_date_df_after_8b_masking_Aug10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 first_author_df updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing first_country with None:\n",
    "\n",
    "def valid_first_country_picker(cnt_list):\n",
    "    for cntry in cnt_list:\n",
    "        if cntry == None:\n",
    "            pass\n",
    "        else:\n",
    "            return cntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_first_author_df[\"first_country\"] = q1_first_author_df.country.map(valid_first_country_picker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check & drop for prob:\n",
    "q1_first_author_df = q1_first_author_df[q1_first_author_df.first_country.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_first_author_df.drop(\"consider\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Adding Country to date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_mapper(auth_list):\n",
    "    cntry_list = list()\n",
    "    for auth in auth_list:\n",
    "        cntry_list.append(q1_first_author_df.loc[auth,\"first_country\"])\n",
    "    return cntry_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[\"country_list\"] = q1_date_df.AUTH_ID.map(country_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 date_df timedelta updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df[\"time_delta\"] = (q1_date_df.Published_datetime - q1_date_df.Submitted_datetime).map(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_date_df = q1_date_df[q1_date_df.time_delta < 3650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is part of the graphing and is not included in this nb(can be included in the future if necessary)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d6808e9ba8815475743150367720cad3673ac2b3f4957dc753295ba7ac37a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
